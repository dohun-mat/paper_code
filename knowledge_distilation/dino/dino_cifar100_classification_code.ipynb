{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5ab2e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models as torchvision_models\n",
    "\n",
    "import utils\n",
    "import vision_transformer as vits\n",
    "from vision_transformer import DINOHead\n",
    "\n",
    "import easydict\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "92350410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "63b9f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "     # Model parameters\n",
    "    \"arch\" : 'vit_small',\n",
    "    'patch_size' : 16,\n",
    "    'out_dim' : 100,\n",
    "#     'out_dim' : 65536,\n",
    "    'norm_last_layer' : True,\n",
    "    'momentum_teacher' : 0.996,\n",
    "    'use_bn_in_head' : False,\n",
    "    # Temperature teacher parameters\n",
    "    'warmup_teacher_temp' : 0.04,\n",
    "    'teacher_temp' : 0.04,\n",
    "    'warmup_teacher_temp_epochs' : 0,\n",
    "    # Training/Optimization parameters\n",
    "    'use_fp16' : True,\n",
    "    'weight_decay' : 0.04,\n",
    "    'weight_decay_end' : 0.4,\n",
    "    'clip_grad' : 3.0,\n",
    "    'batch_size_per_gpu' : 64,\n",
    "    'epochs' : 100,\n",
    "    'freeze_last_layer' : 1,\n",
    "    'lr' : 0.0005,\n",
    "    'warmup_epochs' : 10,\n",
    "    'min_lr' : 1e-6,\n",
    "    'optimizer' : 'adamw',\n",
    "    'drop_path_rate' : 0.1,\n",
    "    # Multi-crop parameters\n",
    "    'global_crops_scale': (0.4, 1.),\n",
    "    'local_crops_number' : 8,\n",
    "    'local_crops_scale' : (0.05, 0.4),\n",
    "     # Misc\n",
    "    'data_path' : 'C:\\\\Users\\\\dhkim\\\\Desktop\\\\directory\\\\imagenet_data',\n",
    "    'output_dir' : 'C:\\\\Users\\\\dhkim\\\\Desktop\\\\directory\\\\dino-main/main_pth',\n",
    "    'saveckp_freq' : 20,\n",
    "    'seed' : 0,\n",
    "    'num_workers' : 0,\n",
    "#     'dist_url' : 'env://'\n",
    "    'local_rank' : 0,\n",
    "    'gpu' : device\n",
    "   })\n",
    "\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "53cac45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_random_seeds(args.seed)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "77878752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmentationDINO(object):\n",
    "    def __init__(self, global_crops_scale, local_crops_scale, local_crops_number):\n",
    "        flip_and_color_jitter = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "        ])\n",
    "        normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "\n",
    "        # first global crop\n",
    "        self.global_transfo1 = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n",
    "            flip_and_color_jitter,\n",
    "            utils.GaussianBlur(1.0),\n",
    "            normalize,\n",
    "        ])\n",
    "        # second global crop\n",
    "        self.global_transfo2 = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n",
    "            flip_and_color_jitter,\n",
    "            utils.GaussianBlur(0.1),\n",
    "            utils.Solarization(0.2),\n",
    "            normalize,\n",
    "        ])\n",
    "        # transformation for the local small crops\n",
    "        self.local_crops_number = local_crops_number\n",
    "        self.local_transfo = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(96, scale=local_crops_scale, interpolation=Image.BICUBIC),\n",
    "            flip_and_color_jitter,\n",
    "            utils.GaussianBlur(p=0.5),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    def __call__(self, image):\n",
    "        crops = []\n",
    "        crops.append(self.global_transfo1(image))\n",
    "        crops.append(self.global_transfo2(image))\n",
    "        for _ in range(self.local_crops_number):\n",
    "            crops.append(self.local_transfo(image))\n",
    "        return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e9290817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOLoss(nn.Module):\n",
    "    def __init__(self, out_dim, ncrops, warmup_teacher_temp, teacher_temp,\n",
    "                 warmup_teacher_temp_epochs, nepochs, student_temp=0.1,\n",
    "                 center_momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.student_temp = student_temp\n",
    "        self.center_momentum = center_momentum\n",
    "        self.ncrops = ncrops\n",
    "        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n",
    "        # we apply a warm up for the teacher temperature because\n",
    "        # a too high temperature makes the training instable at the beginning\n",
    "        self.teacher_temp_schedule = np.concatenate((\n",
    "            np.linspace(warmup_teacher_temp,\n",
    "                        teacher_temp, warmup_teacher_temp_epochs),\n",
    "            np.ones(nepochs - warmup_teacher_temp_epochs) * teacher_temp\n",
    "        ))\n",
    "        \n",
    "\n",
    "    def forward(self, student_output, teacher_output, epoch, target_labels):\n",
    "        \"\"\"\n",
    "        Cross-entropy between softmax outputs of the teacher and student networks.\n",
    "        \"\"\"\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        student_out = student_output / self.student_temp\n",
    "#         print(\"student_out :\", student_out)\n",
    "#         print(\"student_out_shape :\", student_out.shape)        \n",
    "        student_out = student_out.chunk(self.ncrops)\n",
    "#         for i, chunk in enumerate(student_out):\n",
    "#             print(f\"Chunk {i} shape: {chunk.shape}\")\n",
    "        \n",
    "\n",
    "        # teacher centering and sharpening\n",
    "        temp = self.teacher_temp_schedule[epoch]\n",
    "        teacher_out = F.softmax((teacher_output - self.center) / temp, dim=-1)\n",
    "#         print(\"teacher_out : \", teacher_out)\n",
    "#         print(\"teacher_out_shape : \", teacher_out.shape)\n",
    "        teacher_out = teacher_out.detach().chunk(2)\n",
    "\n",
    "        total_loss = 0\n",
    "        n_loss_terms = 0\n",
    "        correct_predictions = 0  # Initialize correct_predictions variable\n",
    "        total_predictions = 0\n",
    "\n",
    "        for iq, q in enumerate(teacher_out):\n",
    "            for v in range(len(student_out)):\n",
    "                if v == iq:\n",
    "                    # we skip cases where student and teacher operate on the same view\n",
    "                    continue\n",
    "                loss = criterion(student_out[v], target_labels)\n",
    "                total_loss += loss\n",
    "                n_loss_terms += 1\n",
    "        total_loss /= n_loss_terms\n",
    "        self.update_center(teacher_output)\n",
    "        \n",
    "         # Calculate accuracy\n",
    "        _, predicted_labels = torch.max(student_out[v], dim=1)\n",
    "        print(\"predicted_labels :\",predicted_labels )\n",
    "        correct_predictions += (predicted_labels == target_labels).sum().item()\n",
    "        total_predictions += target_labels.size(0)\n",
    "        accuracy = correct_predictions / total_predictions *100\n",
    "        print(\"accuracy :\", accuracy)\n",
    "        \n",
    "        return total_loss, accuracy\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_center(self, teacher_output):\n",
    "        \"\"\"\n",
    "        Update center used for teacher output.\n",
    "        \"\"\"\n",
    "        batch_center = torch.sum(teacher_output, dim=0, keepdim=True)\n",
    "#         dist.all_reduce(batch_center)\n",
    "        batch_center = batch_center / (len(teacher_output))\n",
    "\n",
    "        # ema update\n",
    "        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6a907ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Data loaded: there are 50000 images.\n"
     ]
    }
   ],
   "source": [
    "transform = DataAugmentationDINO(\n",
    "    args.global_crops_scale,\n",
    "    args.local_crops_scale,\n",
    "    args.local_crops_number,\n",
    ")\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR100(root='../data', train=True,\n",
    "                                download=True, transform=transform)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=args.batch_size_per_gpu,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "print(f\"Data loaded: there are {len(dataset)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1cb9ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = vits.__dict__[args.arch](\n",
    "    patch_size=args.patch_size,\n",
    "    drop_path_rate=args.drop_path_rate,  # stochastic depth\n",
    ")\n",
    "teacher = vits.__dict__[args.arch](patch_size=args.patch_size)\n",
    "embed_dim = student.embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c1b08d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = utils.MultiCropWrapper(\n",
    "    student, DINOHead(\n",
    "    embed_dim,\n",
    "    args.out_dim,\n",
    "    use_bn=args.use_bn_in_head,\n",
    "    norm_last_layer=args.norm_last_layer,\n",
    "))\n",
    "teacher = utils.MultiCropWrapper(\n",
    "    teacher,\n",
    "    DINOHead(embed_dim, args.out_dim, args.use_bn_in_head),\n",
    ")\n",
    "# fc_input_dim = 65536  # 이전 레이어의 출력 차원\n",
    "# fc_output_dim = 100  # 원하는 출력 차원\n",
    "\n",
    "# # 마지막 FC 레이어 추가\n",
    "# student.fc = nn.Linear(fc_input_dim, fc_output_dim)\n",
    "# teacher.fc = nn.Linear(fc_input_dim, fc_output_dim)\n",
    "\n",
    "\n",
    "\n",
    "# move networks to gpu\n",
    "student, teacher = student.cuda(), teacher.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6ea3ab0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if utils.has_batchnorms(student):\n",
    "    student = nn.SyncBatchNorm.convert_sync_batchnorm(student)\n",
    "    teacher = nn.SyncBatchNorm.convert_sync_batchnorm(teacher)\n",
    "\n",
    "    # we need DDP wrapper to have synchro batch norms working...\n",
    "    teacher = nn.parallel.DistributedDataParallel(teacher, device_ids=[args.gpu])\n",
    "    teacher_without_ddp = teacher.module\n",
    "else:\n",
    "    # teacher_without_ddp and teacher are the same thing\n",
    "    teacher_without_ddp = teacher\n",
    "student.to(args.gpu)\n",
    "# teacher and student start with the same weights\n",
    "teacher_without_ddp.load_state_dict(student.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "30cf493e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student and Teacher are built: they are both vit_small network.\n"
     ]
    }
   ],
   "source": [
    "for p in teacher.parameters():\n",
    "    p.requires_grad = False\n",
    "print(f\"Student and Teacher are built: they are both {args.arch} network.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7576dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_loss = DINOLoss(\n",
    "        args.out_dim,\n",
    "        args.local_crops_number + 2,  # total number of crops = 2 global crops + local_crops_number\n",
    "        args.warmup_teacher_temp,\n",
    "        args.teacher_temp,\n",
    "        args.warmup_teacher_temp_epochs,\n",
    "        args.epochs,\n",
    "    ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "26028ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_groups = utils.get_params_groups(student)\n",
    "if args.optimizer == \"adamw\":\n",
    "    optimizer = torch.optim.AdamW(params_groups)  # to use with ViTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8c58c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mixed precision training\n",
    "fp16_scaler = None\n",
    "if args.use_fp16:\n",
    "    fp16_scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f5755f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss, optimizer and schedulers ready.\n"
     ]
    }
   ],
   "source": [
    "lr_schedule = utils.cosine_scheduler(\n",
    "    args.lr * (args.batch_size_per_gpu * utils.get_world_size()) / 256.,  # linear scaling rule\n",
    "    args.min_lr,\n",
    "    args.epochs, len(data_loader),\n",
    "    warmup_epochs=args.warmup_epochs,\n",
    ")\n",
    "wd_schedule = utils.cosine_scheduler(\n",
    "    args.weight_decay,\n",
    "    args.weight_decay_end,\n",
    "    args.epochs, len(data_loader),\n",
    ")\n",
    "# momentum parameter is increased to 1. during training with a cosine schedule\n",
    "momentum_schedule = utils.cosine_scheduler(args.momentum_teacher, 1, args.epochs, len(data_loader))\n",
    "print(f\"Loss, optimizer and schedulers ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "583d6c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiCropWrapper(\n",
      "  (backbone): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): DropPath()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (head): DINOHead(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=2048, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "      (3): GELU(approximate='none')\n",
      "      (4): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    )\n",
      "    (last_layer): Linear(in_features=256, out_features=100, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b2f733b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiCropWrapper(\n",
      "  (backbone): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (head): DINOHead(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=2048, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "      (3): GELU(approximate='none')\n",
      "      (4): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    )\n",
      "    (last_layer): Linear(in_features=256, out_features=100, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "084d02fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DINO training !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                        | 0/100 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A/tmp/ipykernel_104050/430633712.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets).to(device)\n",
      "\n",
      "1it [00:01,  1.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이터레이션: 0\n",
      "predicted_labels : tensor([57,  8, 61,  8, 62,  8, 47,  8,  8, 62,  8,  8, 57, 42, 62, 15, 72, 98,\n",
      "        62, 42,  8, 87,  7, 57, 57, 19,  8, 36,  7, 70, 62, 70,  8, 15, 87,  8,\n",
      "        37,  8,  8,  8,  6,  8,  8, 39, 61,  8,  8, 44,  8, 62,  8, 72, 57,  8,\n",
      "         8, 96, 87,  8, 87,  8, 71,  8,  8, 38], device='cuda:0')\n",
      "accuracy : 0.0\n",
      "loss :  (tensor(4.8090, device='cuda:0', grad_fn=<DivBackward0>), 0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:03,  1.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([96,  6,  5, 61, 38, 96, 93, 25, 93, 71, 49, 52,  8, 42, 87, 62, 62, 62,\n",
      "        62, 87, 99, 28,  8, 86,  8, 42, 44, 62,  8,  8,  8, 30,  7,  6, 42, 61,\n",
      "        87, 78, 52, 57,  8, 87, 15, 71, 25, 44, 62, 83,  5,  5, 52, 87, 84, 42,\n",
      "        62, 61, 84,  8,  7,  5,  8, 52, 78,  5], device='cuda:0')\n",
      "accuracy : 0.015625\n",
      "loss :  (tensor(4.7084, device='cuda:0', grad_fn=<DivBackward0>), 0.015625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:04,  1.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([ 5, 49,  8, 87, 49, 92,  8, 52, 87, 78, 16, 84, 85, 93,  8,  6, 84, 19,\n",
      "        42, 42, 87,  8,  6,  8, 28, 42, 42, 52, 57, 30,  8,  8, 49, 72, 15, 52,\n",
      "        61, 61,  7,  8,  8,  6,  8, 85, 71,  8,  7,  8, 93, 19, 61, 19,  7,  8,\n",
      "        88, 78, 27, 28, 42,  8, 44,  8,  8,  8], device='cuda:0')\n",
      "accuracy : 0.03125\n",
      "loss :  (tensor(4.7347, device='cuda:0', grad_fn=<DivBackward0>), 0.03125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [00:06,  1.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([86,  8, 44, 62, 62,  8,  8,  8, 84, 25,  8, 87, 52, 87,  5,  8, 25, 57,\n",
      "        47, 36,  8,  8,  8, 57, 88, 28, 84,  8,  8, 39, 84,  8, 96, 84, 70,  8,\n",
      "         8, 19, 62, 44, 15,  8, 57, 52,  8, 42,  7,  8,  6,  8, 44, 72, 87, 27,\n",
      "        91,  3,  8, 62, 62, 52, 71, 62, 37,  1], device='cuda:0')\n",
      "accuracy : 0.015625\n",
      "loss :  (tensor(4.7428, device='cuda:0', grad_fn=<DivBackward0>), 0.015625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [00:07,  1.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([ 8, 52, 52, 61, 28, 61,  8,  8, 78, 62, 42,  8, 19, 42, 95,  1, 72, 87,\n",
      "        72, 42,  1, 62, 63,  8, 57,  8,  7,  8,  5, 62, 44,  8,  8, 96,  3, 93,\n",
      "         8, 52, 24,  8, 15, 62,  8, 56, 84,  7, 72,  5, 30, 19, 52, 61, 42,  8,\n",
      "        57,  8, 25,  8,  5,  3, 27, 87,  7, 78], device='cuda:0')\n",
      "accuracy : 0.015625\n",
      "loss :  (tensor(4.7926, device='cuda:0', grad_fn=<DivBackward0>), 0.015625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [00:09,  1.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([ 8,  8, 62, 57, 72,  7,  8,  6,  7, 42, 37, 49, 96, 22, 52, 15,  7, 42,\n",
      "        71, 87, 62,  8, 87, 62,  8, 81,  8, 52, 16, 15,  8, 93, 93, 93, 42, 93,\n",
      "         7, 39,  8, 24, 66, 45, 66, 87,  8, 30, 91, 87,  8, 84, 52, 79, 57, 52,\n",
      "         8, 62, 47, 44, 33,  7, 87, 52, 49,  1], device='cuda:0')\n",
      "accuracy : 0.0625\n",
      "loss :  (tensor(4.7435, device='cuda:0', grad_fn=<DivBackward0>), 0.0625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [00:11,  1.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([19, 99,  8,  7,  8,  6, 27, 62, 57, 42, 52, 15, 87, 24,  8, 37, 61, 62,\n",
      "        39,  8,  8,  8, 45, 49, 85, 62, 93,  7, 74, 30,  6, 52, 27, 49, 36,  8,\n",
      "         8, 84,  8, 49, 22,  8, 25, 57, 52,  8, 19,  8, 84,  8, 52, 92,  8, 52,\n",
      "        99, 30,  6, 76, 45, 84, 72, 87,  8, 62], device='cuda:0')\n",
      "accuracy : 0.015625\n",
      "loss :  (tensor(4.7885, device='cuda:0', grad_fn=<DivBackward0>), 0.015625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8it [00:12,  1.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([ 8, 61, 42, 85, 84, 42,  8,  8, 15, 98,  8, 19, 85,  7, 62, 62,  6, 19,\n",
      "        96,  8,  8, 93, 44,  6,  6,  8, 74,  8, 25, 90, 70, 62, 47, 39,  8,  8,\n",
      "        87, 27, 84,  8, 90, 62, 62, 47, 44, 44,  8, 78, 62, 52, 93,  6, 96, 27,\n",
      "         8, 49, 62,  8,  8, 87, 36,  8, 83,  8], device='cuda:0')\n",
      "accuracy : 0.03125\n",
      "loss :  (tensor(4.7789, device='cuda:0', grad_fn=<DivBackward0>), 0.03125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [00:14,  1.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([62, 57, 19, 24, 84, 28,  8,  8, 38, 57,  6, 42, 61,  8, 52, 15, 76, 38,\n",
      "        87, 87, 93, 44, 16, 78, 38,  8, 15, 62, 62, 57, 84,  8, 15, 49, 61, 15,\n",
      "        47, 27, 96,  8, 42, 57, 16, 52,  8,  8,  8, 72, 49, 57, 57, 78,  8, 42,\n",
      "        61, 30, 84,  8,  8,  8,  8,  8,  8, 38], device='cuda:0')\n",
      "accuracy : 0.0\n",
      "loss :  (tensor(4.6963, device='cuda:0', grad_fn=<DivBackward0>), 0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [00:15,  1.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([15, 21, 44, 15,  6, 42, 78,  8, 62, 84, 42,  8, 56, 93,  5, 39, 44, 42,\n",
      "        19, 52, 30,  7,  8,  6, 91, 87,  7, 57, 19, 52,  8,  8, 25, 52, 62, 87,\n",
      "        52, 85,  8, 42,  8,  8, 78, 52,  6, 15, 38,  8, 62, 36, 96, 47,  2, 70,\n",
      "        96,  8, 93, 52,  7,  8,  8, 52, 19,  8], device='cuda:0')\n",
      "accuracy : 0.03125\n",
      "loss :  (tensor(4.8046, device='cuda:0', grad_fn=<DivBackward0>), 0.03125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [00:17,  1.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이터레이션: 10\n",
      "predicted_labels : tensor([ 7, 57, 93, 16, 52,  8,  6, 42, 42, 57, 99, 42,  8, 61, 42, 62, 52, 49,\n",
      "        15,  8, 52, 52,  6,  8, 52, 38, 19, 62,  8, 92,  8,  5, 75, 61,  8, 42,\n",
      "        42,  8, 93, 52,  3, 30,  8, 27,  8, 52, 62, 71,  8, 62,  8, 96, 30, 96,\n",
      "        19, 62,  8,  8, 52, 38,  8, 15, 62,  8], device='cuda:0')\n",
      "accuracy : 0.015625\n",
      "loss :  (tensor(4.6950, device='cuda:0', grad_fn=<DivBackward0>), 0.015625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "12it [00:18,  1.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([27,  1,  8, 85, 62,  8, 30,  8, 87,  7, 87, 36, 52, 52, 57,  8,  8,  8,\n",
      "        36,  8,  8, 96, 47,  8, 84, 30, 27, 52, 87, 47, 62, 37, 27,  8, 42, 78,\n",
      "         8,  8, 42,  8, 42,  8, 96, 76, 42, 87, 62,  6,  7,  8, 44, 45, 87,  8,\n",
      "         8,  8, 96, 71, 79, 44,  8, 47,  8, 62], device='cuda:0')\n",
      "accuracy : 0.046875\n",
      "loss :  (tensor(4.7714, device='cuda:0', grad_fn=<DivBackward0>), 0.046875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "13it [00:20,  1.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([49,  8, 52,  8, 15, 15, 88, 92,  8, 19, 42,  8, 42,  1,  1, 30, 49, 70,\n",
      "         8, 38, 93, 42, 52, 99, 59, 87,  8,  3, 87,  7, 19, 30, 61, 25,  8, 62,\n",
      "         8, 87, 24,  8, 62, 62, 87, 25,  8, 27,  8, 30, 87, 61, 90, 24, 85, 93,\n",
      "        39, 28, 15, 84, 91,  8,  6, 99, 36,  8], device='cuda:0')\n",
      "accuracy : 0.03125\n",
      "loss :  (tensor(4.7010, device='cuda:0', grad_fn=<DivBackward0>), 0.03125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "14it [00:21,  1.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([19, 44, 87,  5,  8, 37,  8, 57,  5, 42,  8, 61, 96, 52, 52, 52,  8, 87,\n",
      "         8,  8, 84, 61, 42, 19, 57, 78,  8, 57, 42, 57,  8, 62,  8,  8, 42, 52,\n",
      "        87,  8, 36, 96, 61, 84, 98, 71, 57, 49, 66, 42, 30,  8, 92, 42, 42,  8,\n",
      "         8,  8,  8,  3, 49, 52, 45, 24,  2, 61], device='cuda:0')\n",
      "accuracy : 0.0625\n",
      "loss :  (tensor(4.7523, device='cuda:0', grad_fn=<DivBackward0>), 0.0625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "15it [00:22,  1.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([96, 78,  8, 49,  8, 84,  8,  8,  8,  8, 93, 30,  8,  6, 44, 87, 57,  8,\n",
      "        44, 93, 93, 28, 33, 15, 62,  8,  8,  8, 84, 52, 90, 57, 87, 70,  8,  7,\n",
      "        62, 52,  8, 87, 57,  8,  8, 62,  8,  8,  7, 84, 19,  8, 30, 19, 87, 72,\n",
      "        57, 62, 42, 71, 62,  8, 85,  8, 52,  6], device='cuda:0')\n",
      "accuracy : 0.015625\n",
      "loss :  (tensor(4.7418, device='cuda:0', grad_fn=<DivBackward0>), 0.015625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "16it [00:24,  1.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([ 8, 39, 71,  6,  8,  8, 36, 44, 87, 37, 62,  5, 42, 80, 33,  8, 30, 42,\n",
      "         8,  8, 85, 71, 59,  8, 62, 42, 52, 87,  8, 38,  6, 25, 39, 15, 59, 62,\n",
      "        47, 42,  8, 96, 57, 92, 42,  8, 52, 61, 42,  5, 57,  8, 44,  6, 52,  8,\n",
      "         6,  8,  8, 36, 42, 52, 87, 62, 20,  8], device='cuda:0')\n",
      "accuracy : 0.0\n",
      "loss :  (tensor(4.7876, device='cuda:0', grad_fn=<DivBackward0>), 0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "17it [00:25,  1.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([ 8, 15, 84, 52, 79, 96, 93, 52, 96,  8, 84, 84, 15,  8,  6, 62, 62, 19,\n",
      "        72,  5, 42, 87, 52,  8, 42,  8, 87, 96, 27,  6, 72,  8, 39,  7, 30,  7,\n",
      "         7,  2,  8, 15, 91, 30, 47, 87, 42,  3,  6, 72, 30,  8, 57, 62, 52, 52,\n",
      "         6,  8, 62, 27, 92, 61, 52, 42, 49,  3], device='cuda:0')\n",
      "accuracy : 0.0\n",
      "loss :  (tensor(4.8719, device='cuda:0', grad_fn=<DivBackward0>), 0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "18it [00:26,  1.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([92,  8, 98,  8,  8,  8, 85, 98, 62, 84, 44, 52, 57, 49, 78, 52, 78, 44,\n",
      "        42, 62, 62,  8, 62,  8, 52, 49,  6, 15,  8, 70, 45, 52,  8, 57, 62, 62,\n",
      "        42,  8, 86, 24, 72, 74,  8, 15, 61,  8,  8, 16, 74,  8, 52, 52,  1,  6,\n",
      "         8,  8, 52, 42, 78, 42, 96, 57, 96, 62], device='cuda:0')\n",
      "accuracy : 0.0\n",
      "loss :  (tensor(4.7498, device='cuda:0', grad_fn=<DivBackward0>), 0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "19it [00:28,  1.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([ 8, 42, 24,  8, 84, 62,  8, 72,  8, 15,  8,  3, 44, 72, 92, 62, 96,  8,\n",
      "        96,  8,  3,  8, 44, 42, 61, 27, 78, 96, 19, 75, 61, 47,  8, 42,  8, 39,\n",
      "        57,  8, 42, 47, 61, 42,  8,  8, 37, 19, 42, 93, 87, 78,  8, 36, 61, 44,\n",
      "         8,  8, 84, 96, 30, 62, 62,  8, 61, 96], device='cuda:0')\n",
      "accuracy : 0.015625\n",
      "loss :  (tensor(4.8473, device='cuda:0', grad_fn=<DivBackward0>), 0.015625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20it [00:29,  1.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([42, 33,  8, 85, 52, 91, 36,  8, 39,  8,  5, 96, 19,  8, 92, 30, 44, 52,\n",
      "         8, 62,  8, 52, 57,  3, 15,  8, 96, 30, 57,  8, 52, 42, 36, 33,  8,  7,\n",
      "        62,  8, 47,  8, 28,  8, 30, 96, 47, 44, 72,  6, 19, 52, 62, 62, 52,  8,\n",
      "        19, 42,  8, 42, 36, 30, 42, 84, 30, 27], device='cuda:0')\n",
      "accuracy : 0.015625\n",
      "loss :  (tensor(4.7906, device='cuda:0', grad_fn=<DivBackward0>), 0.015625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21it [00:30,  1.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이터레이션: 20\n",
      "predicted_labels : tensor([52, 52, 84, 52, 61, 27, 15,  6, 19, 62, 57, 57, 96, 96, 85, 42, 96,  7,\n",
      "        79,  6,  8, 39, 70, 38, 52,  8, 57, 42,  7,  8, 30, 56,  8,  8,  8, 15,\n",
      "        61, 93, 93, 70, 42,  8, 84,  8, 62, 45,  8, 47, 93,  8, 30, 25, 75, 84,\n",
      "        62,  8, 70, 96, 62, 87, 57, 84, 49, 96], device='cuda:0')\n",
      "accuracy : 0.015625\n",
      "loss :  (tensor(4.7631, device='cuda:0', grad_fn=<DivBackward0>), 0.015625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22it [00:32,  1.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([93, 57, 62, 87,  8, 52, 19, 52, 93, 19,  8,  8, 84, 57,  8,  6,  8, 44,\n",
      "        25, 87, 57, 52, 62,  5, 49, 62,  8, 92,  8, 52, 52, 52, 44, 87, 96, 44,\n",
      "         7, 44,  8,  8,  8, 42, 61,  8,  8, 57, 84, 52,  2, 52, 61, 84, 44, 47,\n",
      "        71, 39, 75,  8, 59,  8, 15, 76,  6, 62], device='cuda:0')\n",
      "accuracy : 0.03125\n",
      "loss :  (tensor(4.7454, device='cuda:0', grad_fn=<DivBackward0>), 0.03125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "23it [00:33,  1.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([ 1, 84,  8, 44, 52, 27,  8, 96, 19, 39, 52, 44,  8,  8, 52, 36, 62,  8,\n",
      "        96, 78, 19, 61, 87, 27, 99,  8,  8, 72,  6,  8, 42,  8, 27,  8, 42, 36,\n",
      "        62,  8, 57, 44,  8, 96,  6, 62,  8,  8, 48,  8, 96, 91, 62, 52, 36, 61,\n",
      "         3, 57, 62, 91, 42, 56, 52, 52, 87, 22], device='cuda:0')\n",
      "accuracy : 0.0\n",
      "loss :  (tensor(4.8048, device='cuda:0', grad_fn=<DivBackward0>), 0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "24it [00:34,  1.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([96, 59, 70, 25, 52,  8,  8, 87, 96, 57, 62, 96, 39, 49, 36, 19,  8,  8,\n",
      "        52, 52,  8,  6,  8,  8,  8, 57,  8, 36, 48, 45, 52, 85, 30,  8, 96, 93,\n",
      "         8, 71, 87, 62, 96,  6, 61, 66, 57, 84, 85, 62, 85, 62,  8, 72,  8, 62,\n",
      "         5, 15,  5, 52, 52, 93, 62, 45,  8, 30], device='cuda:0')\n",
      "accuracy : 0.03125\n",
      "loss :  (tensor(4.7044, device='cuda:0', grad_fn=<DivBackward0>), 0.03125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [00:36,  1.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([ 8, 42, 62,  7, 84, 87,  8, 93, 36, 66, 42,  8,  8, 19, 37, 72,  8,  3,\n",
      "        30,  8, 57, 92,  8, 36, 52, 61, 52,  8,  5, 42, 45, 62, 52, 39, 72, 28,\n",
      "        52, 49, 42,  8,  1,  8, 24, 96,  8, 91,  8, 52, 38, 52,  8, 62, 85, 52,\n",
      "         8, 76, 91,  5, 99, 96,  8, 27,  8,  8], device='cuda:0')\n",
      "accuracy : 0.015625\n",
      "loss :  (tensor(4.7086, device='cuda:0', grad_fn=<DivBackward0>), 0.015625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "26it [00:37,  1.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([72, 62, 87, 44, 45, 42,  8, 84,  1,  8,  8,  8, 52,  8,  8, 40, 62, 52,\n",
      "        42, 39,  7, 30, 42, 42, 19, 19,  8, 52, 52, 42, 63,  8,  8, 42,  5,  8,\n",
      "        44, 47, 91, 96, 92, 62,  6, 87,  8,  7, 81, 93, 49, 52,  8, 42, 62, 71,\n",
      "        62,  6, 42, 62,  8, 36,  8,  8, 44, 42], device='cuda:0')\n",
      "accuracy : 0.03125\n",
      "loss :  (tensor(4.7863, device='cuda:0', grad_fn=<DivBackward0>), 0.03125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "27it [00:38,  1.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([61, 93, 70, 96, 84, 84, 96, 96, 27, 62, 85,  5, 15, 49,  8, 72, 42,  7,\n",
      "        97,  6, 84, 52, 70, 84, 91,  8,  8,  8,  8, 87, 27, 38,  7, 84, 19, 38,\n",
      "        52, 93, 15, 33, 62,  8,  8, 90, 87, 61, 39, 52, 84,  3,  1,  8, 42, 62,\n",
      "        62,  1, 62,  8,  8, 87,  8, 52, 15, 62], device='cuda:0')\n",
      "accuracy : 0.015625\n",
      "loss :  (tensor(4.7751, device='cuda:0', grad_fn=<DivBackward0>), 0.015625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "28it [00:40,  1.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels : tensor([57, 52, 52, 87,  8,  8,  8,  8, 76, 87,  8, 38, 93, 52,  8, 44, 52, 44,\n",
      "        84, 47,  8, 85,  8, 45, 71,  8, 96, 52,  7,  8, 96,  8, 93, 62,  6, 44,\n",
      "        52, 24, 76, 61, 91,  8, 87,  8,  4, 59, 49,  8, 38,  8, 57, 62, 30, 37,\n",
      "        90, 61,  7, 90,  8, 62,  8, 44, 38, 87], device='cuda:0')\n",
      "accuracy : 0.015625\n",
      "loss :  (tensor(4.8214, device='cuda:0', grad_fn=<DivBackward0>), 0.015625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [00:40,  1.46s/it]\n",
      "  0%|                                                        | 0/100 [00:40<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting DINO training !\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(start_epoch, args\u001b[38;5;241m.\u001b[39mepochs)):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m it, (images, target_label) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(data_loader)):         \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#         print(\"Number of images in the list:\", len(images))\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#         for i, image_tensor in enumerate(images):\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#             print(f\"Shape of image {i}: {image_tensor.shape}\")\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#         print(\"Target label size:\", len(target_label))\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m이터레이션: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "Cell \u001b[0;32mIn[101], line 45\u001b[0m, in \u001b[0;36mDataAugmentationDINO.__call__\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     43\u001b[0m crops\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_transfo2(image))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_crops_number):\n\u001b[0;32m---> 45\u001b[0m     crops\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_transfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m crops\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/torchvision/transforms/transforms.py:962\u001b[0m, in \u001b[0;36mRandomResizedCrop.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    955\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be cropped and resized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Randomly cropped and resized image.\u001b[39;00m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 962\u001b[0m     i, j, h, w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mresized_crop(img, i, j, h, w, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation, antialias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mantialias)\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/torchvision/transforms/transforms.py:923\u001b[0m, in \u001b[0;36mRandomResizedCrop.get_params\u001b[0;34m(img, scale, ratio)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_params\u001b[39m(img: Tensor, scale: List[\u001b[38;5;28mfloat\u001b[39m], ratio: List[\u001b[38;5;28mfloat\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    912\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get parameters for ``crop`` for a random sized crop.\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;124;03m        sized crop.\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 923\u001b[0m     _, height, width \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m     area \u001b[38;5;241m=\u001b[39m height \u001b[38;5;241m*\u001b[39m width\n\u001b[1;32m    926\u001b[0m     log_ratio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(torch\u001b[38;5;241m.\u001b[39mtensor(ratio))\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/torchvision/transforms/functional.py:71\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the dimensions of an image as [channels, height, width].\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    List[int]: The image dimensions.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m---> 71\u001b[0m     \u001b[43m_log_api_usage_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_dimensions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mget_dimensions(img)\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/torchvision/utils.py:562\u001b[0m, in \u001b[0;36m_log_api_usage_once\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, FunctionType):\n\u001b[1;32m    561\u001b[0m     name \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 562\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_api_usage_once\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodule\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting DINO training !\")\n",
    "for epoch in tqdm(range(start_epoch, args.epochs)):\n",
    "    for it, (images, target_label) in tqdm(enumerate(data_loader)):         \n",
    "#         print(\"Number of images in the list:\", len(images))\n",
    "        \n",
    "#         for i, image_tensor in enumerate(images):\n",
    "#             print(f\"Shape of image {i}: {image_tensor.shape}\")\n",
    "        \n",
    "#         print(\"Target label size:\", len(target_label))\n",
    "        \n",
    "        if it % 10 == 0:\n",
    "            print(f\"이터레이션: {it}\")\n",
    "        it = len(data_loader) * epoch + it  # global training iteration\n",
    "        for i, param_group in enumerate(optimizer.param_groups):\n",
    "            param_group[\"lr\"] = lr_schedule[it]\n",
    "            if i == 0:  # only the first group is regularized\n",
    "                param_group[\"weight_decay\"] = wd_schedule[it]\n",
    "                \n",
    "        images = [im.cuda(non_blocking=True) for im in images]\n",
    "#         targets = [tl.cuda(non_blocking=True) for tl in target_label]\n",
    "        \n",
    "#         print(len(images))\n",
    "#         print(len(targets))\n",
    "        targets = torch.tensor(targets).to(device)\n",
    "        \n",
    "        ##muti_view_crop 때문에```\n",
    "#                 Number of images in the list: 10\n",
    "#         Shape of image 0: torch.Size([64, 3, 224, 224])\n",
    "#         Shape of image 1: torch.Size([64, 3, 224, 224])\n",
    "#         Shape of image 2: torch.Size([64, 3, 96, 96])\n",
    "#         Shape of image 3: torch.Size([64, 3, 96, 96])\n",
    "#         Shape of image 4: torch.Size([64, 3, 96, 96])\n",
    "#         Shape of image 5: torch.Size([64, 3, 96, 96])\n",
    "#         Shape of image 6: torch.Size([64, 3, 96, 96])\n",
    "#         Shape of image 7: torch.Size([64, 3, 96, 96])\n",
    "#         Shape of image 8: torch.Size([64, 3, 96, 96])\n",
    "#         Shape of image 9: torch.Size([64, 3, 96, 96])\n",
    "#         Target label size: 64\n",
    "#         이런구조임\n",
    "        \n",
    "         \n",
    "        with torch.cuda.amp.autocast(fp16_scaler is not None):\n",
    "            teacher_output = teacher(images[:2])  # only the 2 global views pass through the teacher\n",
    "            student_output = student(images)\n",
    "            loss = dino_loss(student_output, teacher_output, epoch, targets)\n",
    "            print(\"loss : \", loss)\n",
    "            \n",
    "        # student update\n",
    "        optimizer.zero_grad()\n",
    "        param_norms = None\n",
    "        if fp16_scaler is None:\n",
    "            loss.backward()\n",
    "            if args.clip_grad:\n",
    "                param_norms = utils.clip_gradients(student, args.clip_grad)\n",
    "#             utils.cancel_gradients_last_layer(epoch, student,\n",
    "#                                               args.freeze_last_layer)\n",
    "            optimizer.step()\n",
    "    \n",
    "\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            m = momentum_schedule[it]  # momentum parameter\n",
    "            #teacher의 가중치를 student에게 가르쳐줌\n",
    "            for param_q, param_k in zip(student.parameters(), teacher_without_ddp.parameters()):\n",
    "                param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)\n",
    "                \n",
    "    # ============ writing logs ... ============\n",
    "    save_dict = {\n",
    "        'student': student.state_dict(),\n",
    "        'teacher': teacher.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch + 1,\n",
    "        'args': args,\n",
    "        'dino_loss': dino_loss.state_dict(),\n",
    "    }\n",
    "    \n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abf8d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526eb884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
