{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b00081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import argparse\n",
    "import torch.utils.data as data\n",
    "# from data import WiderFaceDetection, detection_collate, preproc, cfg_mnet, cfg_re50\n",
    "# from layers.modules import MultiBoxLoss\n",
    "# from layers.functions.prior_box import PriorBox\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "# from models.retinaface import RetinaFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7826b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa9dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb=500, min_alloc_size_mb=20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d525a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_mnet = {\n",
    "    'name': 'mobilenet0.25',\n",
    "    'min_sizes': [[16, 32], [64, 128], [256, 512]],\n",
    "    'steps': [8, 16, 32],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': False,\n",
    "    'loc_weight': 2.0,\n",
    "    'gpu_train': True,\n",
    "    'batch_size': 32,\n",
    "    'ngpu': 1,\n",
    "    'epoch': 250,\n",
    "    'decay1': 190,\n",
    "    'decay2': 220,\n",
    "    'image_size': 640,\n",
    "    'pretrain': True,\n",
    "    'return_layers': {'stage1': 1, 'stage2': 2, 'stage3': 3},\n",
    "    'in_channel': 32,\n",
    "    'out_channel': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33574f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_re50 = {\n",
    "    'name': 'Resnet50',\n",
    "    'min_sizes': [[16, 32], [64, 128], [256, 512]],\n",
    "    'steps': [8, 16, 32],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': False,\n",
    "    'loc_weight': 2.0,\n",
    "    'gpu_train': True,\n",
    "    'batch_size': 24,\n",
    "    'ngpu': 4,\n",
    "    'epoch': 100,\n",
    "    'decay1': 70,\n",
    "    'decay2': 90,\n",
    "    'image_size': 840,\n",
    "    'pretrain': True,\n",
    "    'return_layers': {'layer2': 1, 'layer3': 2, 'layer4': 3},\n",
    "    'in_channel': 256,\n",
    "    'out_channel': 256\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a1de634",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_convnext = {\n",
    "    'name': 'convnext_large',\n",
    "    'min_sizes': [[16, 32], [64, 128], [256, 512]],\n",
    "    'steps': [8, 16, 32],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': False,\n",
    "    'loc_weight': 2.0,\n",
    "    'gpu_train': True,\n",
    "    'batch_size': 3,\n",
    "    'ngpu': 1,\n",
    "    'epoch': 100,\n",
    "    'decay1': 70,\n",
    "    'decay2': 90,\n",
    "    'image_size': 840,\n",
    "    'pretrain': True,\n",
    "    'return_layers': {'stem': 1, 'stages': 2, 'head' : 3},\n",
    "    'in_channel': 256,\n",
    "    'out_channel': 256\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756b8539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671757c1",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d924ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crop(image, boxes, labels, landm, img_dim):\n",
    "    height, width, _ = image.shape\n",
    "    pad_image_flag = True\n",
    "\n",
    "    for _ in range(250):\n",
    "        \"\"\"\n",
    "        if random.uniform(0, 1) <= 0.2:\n",
    "            scale = 1.0\n",
    "        else:\n",
    "            scale = random.uniform(0.3, 1.0)\n",
    "        \"\"\"\n",
    "        PRE_SCALES = [0.3, 0.45, 0.6, 0.8, 1.0]\n",
    "        scale = random.choice(PRE_SCALES)\n",
    "        short_side = min(width, height)\n",
    "        w = int(scale * short_side)\n",
    "        h = w\n",
    "\n",
    "        if width == w:\n",
    "            l = 0\n",
    "        else:\n",
    "            l = random.randrange(width - w)\n",
    "        if height == h:\n",
    "            t = 0\n",
    "        else:\n",
    "            t = random.randrange(height - h)\n",
    "        roi = np.array((l, t, l + w, t + h))\n",
    "\n",
    "        value = matrix_iof(boxes, roi[np.newaxis])\n",
    "        flag = (value >= 1)\n",
    "        if not flag.any():\n",
    "            continue\n",
    "\n",
    "        centers = (boxes[:, :2] + boxes[:, 2:]) / 2\n",
    "        mask_a = np.logical_and(roi[:2] < centers, centers < roi[2:]).all(axis=1)\n",
    "        boxes_t = boxes[mask_a].copy()\n",
    "        labels_t = labels[mask_a].copy()\n",
    "        landms_t = landm[mask_a].copy()\n",
    "        landms_t = landms_t.reshape([-1, 5, 2])\n",
    "\n",
    "        if boxes_t.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        image_t = image[roi[1]:roi[3], roi[0]:roi[2]]\n",
    "\n",
    "        boxes_t[:, :2] = np.maximum(boxes_t[:, :2], roi[:2])\n",
    "        boxes_t[:, :2] -= roi[:2]\n",
    "        boxes_t[:, 2:] = np.minimum(boxes_t[:, 2:], roi[2:])\n",
    "        boxes_t[:, 2:] -= roi[:2]\n",
    "\n",
    "        # landm\n",
    "        landms_t[:, :, :2] = landms_t[:, :, :2] - roi[:2]\n",
    "        landms_t[:, :, :2] = np.maximum(landms_t[:, :, :2], np.array([0, 0]))\n",
    "        landms_t[:, :, :2] = np.minimum(landms_t[:, :, :2], roi[2:] - roi[:2])\n",
    "        landms_t = landms_t.reshape([-1, 10])\n",
    "\n",
    "\n",
    "\t# make sure that the cropped image contains at least one face > 16 pixel at training image scale\n",
    "        b_w_t = (boxes_t[:, 2] - boxes_t[:, 0] + 1) / w * img_dim\n",
    "        b_h_t = (boxes_t[:, 3] - boxes_t[:, 1] + 1) / h * img_dim\n",
    "        mask_b = np.minimum(b_w_t, b_h_t) > 0.0\n",
    "        boxes_t = boxes_t[mask_b]\n",
    "        labels_t = labels_t[mask_b]\n",
    "        landms_t = landms_t[mask_b]\n",
    "\n",
    "        if boxes_t.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        pad_image_flag = False\n",
    "\n",
    "        return image_t, boxes_t, labels_t, landms_t, pad_image_flag\n",
    "    return image, boxes, labels, landm, pad_image_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e96475ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distort(image):\n",
    "\n",
    "    def _convert(image, alpha=1, beta=0):\n",
    "        tmp = image.astype(float) * alpha + beta\n",
    "        tmp[tmp < 0] = 0\n",
    "        tmp[tmp > 255] = 255\n",
    "        image[:] = tmp\n",
    "\n",
    "    image = image.copy()\n",
    "\n",
    "    if random.randrange(2):\n",
    "\n",
    "        #brightness distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image, beta=random.uniform(-32, 32))\n",
    "\n",
    "        #contrast distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image, alpha=random.uniform(0.5, 1.5))\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        #saturation distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image[:, :, 1], alpha=random.uniform(0.5, 1.5))\n",
    "\n",
    "        #hue distortion\n",
    "        if random.randrange(2):\n",
    "            tmp = image[:, :, 0].astype(int) + random.randint(-18, 18)\n",
    "            tmp %= 180\n",
    "            image[:, :, 0] = tmp\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    else:\n",
    "\n",
    "        #brightness distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image, beta=random.uniform(-32, 32))\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        #saturation distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image[:, :, 1], alpha=random.uniform(0.5, 1.5))\n",
    "\n",
    "        #hue distortion\n",
    "        if random.randrange(2):\n",
    "            tmp = image[:, :, 0].astype(int) + random.randint(-18, 18)\n",
    "            tmp %= 180\n",
    "            image[:, :, 0] = tmp\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        #contrast distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image, alpha=random.uniform(0.5, 1.5))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa61ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mirror(image, boxes, landms):\n",
    "    _, width, _ = image.shape\n",
    "    if random.randrange(2):\n",
    "        image = image[:, ::-1]\n",
    "        boxes = boxes.copy()\n",
    "        boxes[:, 0::2] = width - boxes[:, 2::-2]\n",
    "\n",
    "        # landm\n",
    "        landms = landms.copy()\n",
    "        landms = landms.reshape([-1, 5, 2])\n",
    "        landms[:, :, 0] = width - landms[:, :, 0]\n",
    "        tmp = landms[:, 1, :].copy()\n",
    "        landms[:, 1, :] = landms[:, 0, :]\n",
    "        landms[:, 0, :] = tmp\n",
    "        tmp1 = landms[:, 4, :].copy()\n",
    "        landms[:, 4, :] = landms[:, 3, :]\n",
    "        landms[:, 3, :] = tmp1\n",
    "        landms = landms.reshape([-1, 10])\n",
    "\n",
    "    return image, boxes, landms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8faf443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_to_square(image, rgb_mean, pad_image_flag):\n",
    "    if not pad_image_flag:\n",
    "        return image\n",
    "    height, width, _ = image.shape\n",
    "    long_side = max(width, height)\n",
    "    image_t = np.empty((long_side, long_side, 3), dtype=image.dtype)\n",
    "    image_t[:, :] = rgb_mean\n",
    "    image_t[0:0 + height, 0:0 + width] = image\n",
    "    return image_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2578f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WiderFaceDetection(data.Dataset):\n",
    "    def __init__(self, txt_path, preproc=None):\n",
    "        self.preproc = preproc\n",
    "        self.imgs_path = []\n",
    "        self.words = []\n",
    "        f = open(txt_path,'r')\n",
    "        lines = f.readlines()\n",
    "        isFirst = True\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            line = line.rstrip()\n",
    "            if line.startswith('#'):\n",
    "                if isFirst is True:\n",
    "                    isFirst = False\n",
    "                else:\n",
    "                    labels_copy = labels.copy()\n",
    "                    self.words.append(labels_copy)\n",
    "                    labels.clear()\n",
    "                path = line[2:]\n",
    "                path = txt_path.replace('label.txt','images/') + path\n",
    "                self.imgs_path.append(path)\n",
    "            else:\n",
    "                line = line.split(' ')\n",
    "                label = [float(x) for x in line]\n",
    "                labels.append(label)\n",
    "\n",
    "        self.words.append(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = cv2.imread(self.imgs_path[index])\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        labels = self.words[index]\n",
    "        annotations = np.zeros((0, 15))\n",
    "        if len(labels) == 0:\n",
    "            return annotations\n",
    "        for idx, label in enumerate(labels):\n",
    "            annotation = np.zeros((1, 15))\n",
    "            # bbox\n",
    "            annotation[0, 0] = label[0]  # x1\n",
    "            annotation[0, 1] = label[1]  # y1\n",
    "            annotation[0, 2] = label[0] + label[2]  # x2\n",
    "            annotation[0, 3] = label[1] + label[3]  # y2\n",
    "\n",
    "            # landmarks\n",
    "            annotation[0, 4] = label[4]    # l0_x\n",
    "            annotation[0, 5] = label[5]    # l0_y\n",
    "            annotation[0, 6] = label[7]    # l1_x\n",
    "            annotation[0, 7] = label[8]    # l1_y\n",
    "            annotation[0, 8] = label[10]   # l2_x\n",
    "            annotation[0, 9] = label[11]   # l2_y\n",
    "            annotation[0, 10] = label[13]  # l3_x\n",
    "            annotation[0, 11] = label[14]  # l3_y\n",
    "            annotation[0, 12] = label[16]  # l4_x\n",
    "            annotation[0, 13] = label[17]  # l4_y\n",
    "            if (annotation[0, 4]<0):\n",
    "                annotation[0, 14] = -1\n",
    "            else:\n",
    "                annotation[0, 14] = 1\n",
    "\n",
    "            annotations = np.append(annotations, annotation, axis=0)\n",
    "        target = np.array(annotations)\n",
    "        if self.preproc is not None:\n",
    "            img, target = self.preproc(img, target)\n",
    "\n",
    "        return torch.from_numpy(img), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8024404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_collate(batch):\n",
    "    \"\"\"Custom collate fn for dealing with batches of images that have a different\n",
    "    number of associated object annotations (bounding boxes).\n",
    "\n",
    "    Arguments:\n",
    "        batch: (tuple) A tuple of tensor images and lists of annotations\n",
    "\n",
    "    Return:\n",
    "        A tuple containing:\n",
    "            1) (tensor) batch of images stacked on their 0 dim\n",
    "            2) (list of tensors) annotations for a given image are stacked on 0 dim\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for _, sample in enumerate(batch):\n",
    "        for _, tup in enumerate(sample):\n",
    "            if torch.is_tensor(tup):\n",
    "                imgs.append(tup)\n",
    "            elif isinstance(tup, type(np.empty(0))):\n",
    "                annos = torch.from_numpy(tup).float()\n",
    "                targets.append(annos)\n",
    "\n",
    "    return (torch.stack(imgs, 0), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a09512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resize_subtract_mean(image, insize, rgb_mean):\n",
    "    interp_methods = [cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_NEAREST, cv2.INTER_LANCZOS4]\n",
    "    interp_method = interp_methods[random.randrange(5)]\n",
    "    image = cv2.resize(image, (insize, insize), interpolation=interp_method)\n",
    "    image = image.astype(np.float32)\n",
    "    image -= rgb_mean\n",
    "    return image.transpose(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdeebd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preproc(object):\n",
    "\n",
    "    def __init__(self, img_dim, rgb_means):\n",
    "        self.img_dim = img_dim\n",
    "        self.rgb_means = rgb_means\n",
    "\n",
    "    def __call__(self, image, targets):\n",
    "        assert targets.shape[0] > 0, \"this image does not have gt\"\n",
    "\n",
    "        boxes = targets[:, :4].copy()\n",
    "        labels = targets[:, -1].copy()\n",
    "        landm = targets[:, 4:-1].copy()\n",
    "\n",
    "        image_t, boxes_t, labels_t, landm_t, pad_image_flag = _crop(image, boxes, labels, landm, self.img_dim)\n",
    "        image_t = _distort(image_t)\n",
    "        image_t = _pad_to_square(image_t,self.rgb_means, pad_image_flag)\n",
    "        image_t, boxes_t, landm_t = _mirror(image_t, boxes_t, landm_t)\n",
    "        height, width, _ = image_t.shape\n",
    "        image_t = _resize_subtract_mean(image_t, self.img_dim, self.rgb_means)\n",
    "        boxes_t[:, 0::2] /= width\n",
    "        boxes_t[:, 1::2] /= height\n",
    "\n",
    "        landm_t[:, 0::2] /= width\n",
    "        landm_t[:, 1::2] /= height\n",
    "\n",
    "        labels_t = np.expand_dims(labels_t, 1)\n",
    "        targets_t = np.hstack((boxes_t, landm_t, labels_t))\n",
    "\n",
    "        return image_t, targets_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd91ea",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc63a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_iou(a, b):\n",
    "    \"\"\"\n",
    "    return iou of a and b, numpy version for data augenmentation\n",
    "    \"\"\"\n",
    "    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n",
    "    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n",
    "\n",
    "    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n",
    "    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n",
    "    area_b = np.prod(b[:, 2:] - b[:, :2], axis=1)\n",
    "    return area_i / (area_a[:, np.newaxis] + area_b - area_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb00651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_iof(a, b):\n",
    "    \"\"\"\n",
    "    return iof of a and b, numpy version for data augenmentation\n",
    "    \"\"\"\n",
    "    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n",
    "    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n",
    "\n",
    "    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n",
    "    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n",
    "    return area_i / np.maximum(area_a[:, np.newaxis], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70871498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_form(boxes):\n",
    "    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n",
    "    representation for comparison to point form ground truth data.\n",
    "    Args:\n",
    "        boxes: (tensor) center-size default boxes from priorbox layers.\n",
    "    Return:\n",
    "        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n",
    "    \"\"\"\n",
    "    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n",
    "                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80fbb806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(matched, priors, variances):\n",
    "    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n",
    "    we have matched (based on jaccard overlap) with the prior boxes.\n",
    "    Args:\n",
    "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
    "            Shape: [num_priors, 4].\n",
    "        priors: (tensor) Prior boxes in center-offset form\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        encoded boxes (tensor), Shape: [num_priors, 4]\n",
    "    \"\"\"\n",
    "\n",
    "    # dist b/t match center and prior's center\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n",
    "    # encode variance\n",
    "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
    "    # match wh / prior wh\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
    "    g_wh = torch.log(g_wh) / variances[1]\n",
    "    # return target for smooth_l1_loss\n",
    "    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2202eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_landm(matched, priors, variances):\n",
    "    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n",
    "    we have matched (based on jaccard overlap) with the prior boxes.\n",
    "    Args:\n",
    "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
    "            Shape: [num_priors, 10].\n",
    "        priors: (tensor) Prior boxes in center-offset form\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        encoded landm (tensor), Shape: [num_priors, 10]\n",
    "    \"\"\"\n",
    "\n",
    "    # dist b/t match center and prior's center\n",
    "    matched = torch.reshape(matched, (matched.size(0), 5, 2))\n",
    "    priors_cx = priors[:, 0].unsqueeze(1).expand(matched.size(0), 5).unsqueeze(2)\n",
    "    priors_cy = priors[:, 1].unsqueeze(1).expand(matched.size(0), 5).unsqueeze(2)\n",
    "    priors_w = priors[:, 2].unsqueeze(1).expand(matched.size(0), 5).unsqueeze(2)\n",
    "    priors_h = priors[:, 3].unsqueeze(1).expand(matched.size(0), 5).unsqueeze(2)\n",
    "    priors = torch.cat([priors_cx, priors_cy, priors_w, priors_h], dim=2)\n",
    "    g_cxcy = matched[:, :, :2] - priors[:, :, :2]\n",
    "    # encode variance\n",
    "    g_cxcy /= (variances[0] * priors[:, :, 2:])\n",
    "    # g_cxcy /= priors[:, :, 2:]\n",
    "    g_cxcy = g_cxcy.reshape(g_cxcy.size(0), -1)\n",
    "    # return target for smooth_l1_loss\n",
    "    return g_cxcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08ae2bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(loc, priors, variances):\n",
    "    \"\"\"Decode locations from predictions using priors to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        loc (tensor): location predictions for loc layers,\n",
    "            Shape: [num_priors,4]\n",
    "        priors (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        decoded bounding box predictions\n",
    "    \"\"\"\n",
    "\n",
    "    boxes = torch.cat((\n",
    "        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
    "        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n",
    "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
    "    boxes[:, 2:] += boxes[:, :2]\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41f846e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_landm(pre, priors, variances):\n",
    "    \"\"\"Decode landm from predictions using priors to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        pre (tensor): landm predictions for loc layers,\n",
    "            Shape: [num_priors,10]\n",
    "        priors (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        decoded landm predictions\n",
    "    \"\"\"\n",
    "    landms = torch.cat((priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:],\n",
    "                        ), dim=1)\n",
    "    return landms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b1b5e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(box_a, box_b):\n",
    "    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n",
    "    is simply the intersection over union of two boxes.  Here we operate on\n",
    "    ground truth boxes and default boxes.\n",
    "    E.g.:\n",
    "        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
    "    Args:\n",
    "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n",
    "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n",
    "    Return:\n",
    "        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n",
    "    \"\"\"\n",
    "    inter = intersect(box_a, box_b)\n",
    "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
    "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
    "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
    "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fd9fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(threshold, truths, priors, variances, labels, landms, loc_t, conf_t, landm_t, idx):\n",
    "    \"\"\"Match each prior box with the ground truth box of the highest jaccard\n",
    "    overlap, encode the bounding boxes, then return the matched indices\n",
    "    corresponding to both confidence and location preds.\n",
    "    Args:\n",
    "        threshold: (float) The overlap threshold used when mathing boxes.\n",
    "        truths: (tensor) Ground truth boxes, Shape: [num_obj, 4].\n",
    "        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n",
    "        variances: (tensor) Variances corresponding to each prior coord,\n",
    "            Shape: [num_priors, 4].\n",
    "        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n",
    "        landms: (tensor) Ground truth landms, Shape [num_obj, 10].\n",
    "        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n",
    "        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n",
    "        landm_t: (tensor) Tensor to be filled w/ endcoded landm targets.\n",
    "        idx: (int) current batch index\n",
    "    Return:\n",
    "        The matched indices corresponding to 1)location 2)confidence 3)landm preds.\n",
    "    \"\"\"\n",
    "    # jaccard index\n",
    "    overlaps = jaccard(\n",
    "        truths,\n",
    "        point_form(priors)\n",
    "    )\n",
    "    # (Bipartite Matching)\n",
    "    # [1,num_objects] best prior for each ground truth\n",
    "    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n",
    "\n",
    "    # ignore hard gt\n",
    "    valid_gt_idx = best_prior_overlap[:, 0] >= 0.2\n",
    "    best_prior_idx_filter = best_prior_idx[valid_gt_idx, :]\n",
    "    if best_prior_idx_filter.shape[0] <= 0:\n",
    "        loc_t[idx] = 0\n",
    "        conf_t[idx] = 0\n",
    "        return\n",
    "\n",
    "    # [1,num_priors] best ground truth for each prior\n",
    "    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n",
    "    best_truth_idx.squeeze_(0)\n",
    "    best_truth_overlap.squeeze_(0)\n",
    "    best_prior_idx.squeeze_(1)\n",
    "    best_prior_idx_filter.squeeze_(1)\n",
    "    best_prior_overlap.squeeze_(1)\n",
    "    best_truth_overlap.index_fill_(0, best_prior_idx_filter, 2)  # ensure best prior\n",
    "    # TODO refactor: index  best_prior_idx with long tensor\n",
    "    # ensure every gt matches with its prior of max overlap\n",
    "    for j in range(best_prior_idx.size(0)):     # 判别此anchor是预测哪一个boxes\n",
    "        best_truth_idx[best_prior_idx[j]] = j\n",
    "    matches = truths[best_truth_idx]            # Shape: [num_priors,4] 此处为每一个anchor对应的bbox取出来\n",
    "    conf = labels[best_truth_idx]               # Shape: [num_priors]      此处为每一个anchor对应的label取出来\n",
    "    conf[best_truth_overlap < threshold] = 0    # label as background   overlap<0.35的全部作为负样本\n",
    "    loc = encode(matches, priors, variances)\n",
    "\n",
    "    matches_landm = landms[best_truth_idx]\n",
    "    landm = encode_landm(matches_landm, priors, variances)\n",
    "    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n",
    "    conf_t[idx] = conf  # [num_priors] top class label for each prior\n",
    "    landm_t[idx] = landm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cde6a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(box_a, box_b):\n",
    "    \"\"\" We resize both tensors to [A,B,2] without new malloc:\n",
    "    [A,2] -> [A,1,2] -> [A,B,2]\n",
    "    [B,2] -> [1,B,2] -> [A,B,2]\n",
    "    Then we compute the area of intersect between box_a and box_b.\n",
    "    Args:\n",
    "      box_a: (tensor) bounding boxes, Shape: [A,4].\n",
    "      box_b: (tensor) bounding boxes, Shape: [B,4].\n",
    "    Return:\n",
    "      (tensor) intersection area, Shape: [A,B].\n",
    "    \"\"\"\n",
    "    A = box_a.size(0)\n",
    "    B = box_b.size(0)\n",
    "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n",
    "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b86180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x):\n",
    "    \"\"\"Utility function for computing log_sum_exp while determining\n",
    "    This will be used to determine unaveraged confidence loss across\n",
    "    all examples in a batch.\n",
    "    Args:\n",
    "        x (Variable(tensor)): conf_preds from conf layers\n",
    "    \"\"\"\n",
    "    x_max = x.data.max()\n",
    "    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350ed290",
   "metadata": {},
   "source": [
    "# layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13b3ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "# from utils.box_utils import match, log_sum_exp\n",
    "# from data import cfg_mnet\n",
    "GPU = cfg_mnet['gpu_train']\n",
    "\n",
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"SSD Weighted Loss Function\n",
    "    Compute Targets:\n",
    "        1) Produce Confidence Target Indices by matching  ground truth boxes\n",
    "           with (default) 'priorboxes' that have jaccard index > threshold parameter\n",
    "           (default threshold: 0.5).\n",
    "        2) Produce localization target by 'encoding' variance into offsets of ground\n",
    "           truth boxes and their matched  'priorboxes'.\n",
    "        3) Hard negative mining to filter the excessive number of negative examples\n",
    "           that comes with using a large number of default bounding boxes.\n",
    "           (default negative:positive ratio 3:1)\n",
    "    Objective Loss:\n",
    "        L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
    "        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n",
    "        weighted by α which is set to 1 by cross val.\n",
    "        Args:\n",
    "            c: class confidences =  #face classification loss\n",
    "            l: predicted boxes = #face box regression loss\n",
    "            # Facial landmark regression loss\n",
    "            g: ground truth boxes\n",
    "            N: number of matched default boxes\n",
    "        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, overlap_thresh, prior_for_matching, bkg_label, neg_mining, neg_pos, neg_overlap, encode_target):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = overlap_thresh\n",
    "        self.background_label = bkg_label\n",
    "        self.encode_target = encode_target\n",
    "        self.use_prior_for_matching = prior_for_matching\n",
    "        self.do_neg_mining = neg_mining\n",
    "        self.negpos_ratio = neg_pos\n",
    "        self.neg_overlap = neg_overlap\n",
    "        self.variance = [0.1, 0.2]\n",
    "\n",
    "    def forward(self, predictions, priors, targets):\n",
    "        \"\"\"Multibox Loss\n",
    "        Args:\n",
    "            predictions (tuple): A tuple containing loc preds, conf preds,\n",
    "            and prior boxes from SSD net.\n",
    "                conf shape: torch.size(batch_size,num_priors,num_classes)\n",
    "                loc shape: torch.size(batch_size,num_priors,4)\n",
    "                priors shape: torch.size(num_priors,4)\n",
    "\n",
    "            ground_truth (tensor): Ground truth boxes and labels for a batch,\n",
    "                shape: [batch_size,num_objs,5] (last idx is the label).\n",
    "        \"\"\"\n",
    "        \n",
    "#         print(\"predictions \\n\", predictions)\n",
    "#         print(\"priors \\n\", priors)\n",
    "#         print(\"targets \\n\", targets)\n",
    "        \n",
    "        loc_data, conf_data, landm_data = predictions\n",
    "        priors = priors\n",
    "        num = loc_data.size(0)\n",
    "        num_priors = (priors.size(0))\n",
    "        \n",
    "#         print(\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "#         print(priors.shape)\n",
    "#         print(num)\n",
    "#         print(num_priors)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(\"############################\")\n",
    "#         print(loc_data.shape)\n",
    "#         print(conf_data.shape)\n",
    "#         print(landm_data.shape)\n",
    "        \n",
    "#         print(loc_data)\n",
    "#         print(conf_data)\n",
    "#         print(num)\n",
    "#         print(num_priors)\n",
    "        \n",
    "\n",
    "        # match priors (default boxes) and ground truth boxes\n",
    "        loc_t = torch.Tensor(num, num_priors, 4)\n",
    "        landm_t = torch.Tensor(num, num_priors, 10)\n",
    "        conf_t = torch.LongTensor(num, num_priors)\n",
    "        \n",
    "#         print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "#         print(loc_t.shape)\n",
    "#         print(landm_t.shape)\n",
    "#         print(conf_t.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for idx in range(num):\n",
    "            truths = targets[idx][:, :4].data\n",
    "            labels = targets[idx][:, -1].data\n",
    "            landms = targets[idx][:, 4:14].data\n",
    "            defaults = priors.data\n",
    "            match(self.threshold, truths, defaults, self.variance, labels, landms, loc_t, conf_t, landm_t, idx)\n",
    "        if GPU:\n",
    "            loc_t = loc_t.cuda()\n",
    "            conf_t = conf_t.cuda()\n",
    "            landm_t = landm_t.cuda()\n",
    "\n",
    "        zeros = torch.tensor(0).cuda()\n",
    "        # landm Loss (Smooth L1)\n",
    "        # Shape: [batch,num_priors,10]\n",
    "        pos1 = conf_t > zeros\n",
    "        num_pos_landm = pos1.long().sum(1, keepdim=True)\n",
    "        N1 = max(num_pos_landm.data.sum().float(), 1)\n",
    "        pos_idx1 = pos1.unsqueeze(pos1.dim()).expand_as(landm_data)\n",
    "        landm_p = landm_data[pos_idx1].view(-1, 10)\n",
    "        landm_t = landm_t[pos_idx1].view(-1, 10)\n",
    "        loss_landm = F.smooth_l1_loss(landm_p, landm_t, reduction='sum')\n",
    "\n",
    "\n",
    "        pos = conf_t != zeros\n",
    "        conf_t[pos] = 1\n",
    "\n",
    "        # Localization Loss (Smooth L1)\n",
    "        # Shape: [batch,num_priors,4]\n",
    "        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n",
    "#         print(\"pos_idx\")\n",
    "#         print(pos_idx)\n",
    "        \n",
    "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
    "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
    "\n",
    "        # Compute max conf across batch for hard negative mining\n",
    "        batch_conf = conf_data.view(-1, self.num_classes)\n",
    "        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n",
    "\n",
    "        # Hard Negative Mining\n",
    "        loss_c[pos.view(-1, 1)] = 0 # filter out pos boxes for now\n",
    "        loss_c = loss_c.view(num, -1)\n",
    "        _, loss_idx = loss_c.sort(1, descending=True)\n",
    "        _, idx_rank = loss_idx.sort(1)\n",
    "        num_pos = pos.long().sum(1, keepdim=True)\n",
    "        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n",
    "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
    "\n",
    "        # Confidence Loss Including Positive and Negative Examples\n",
    "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
    "        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1,self.num_classes)\n",
    "        targets_weighted = conf_t[(pos+neg).gt(0)]\n",
    "        loss_c = F.cross_entropy(conf_p, targets_weighted, reduction='sum')\n",
    "\n",
    "        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
    "        N = max(num_pos.data.sum().float(), 1)\n",
    "        loss_l /= N\n",
    "        loss_c /= N\n",
    "        loss_landm /= N1\n",
    "\n",
    "        return loss_l, loss_c, loss_landm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae0c82db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from itertools import product as product\n",
    "\n",
    "class PriorBox(object):\n",
    "    def __init__(self, cfg, image_size=None, phase='train'):\n",
    "        super(PriorBox, self).__init__()\n",
    "        self.min_sizes = cfg['min_sizes']\n",
    "        self.steps = cfg['steps']\n",
    "        self.clip = cfg['clip']\n",
    "        self.image_size = image_size\n",
    "        self.feature_maps = [[ceil(self.image_size[0]/step), ceil(self.image_size[1]/step)] for step in self.steps]\n",
    "        self.name = \"s\"\n",
    "#         print(\"prior\")\n",
    "        \n",
    "#         print(\"min_sizes\")\n",
    "#         print(self.min_sizes)\n",
    "#         print(\"image_size\")\n",
    "#         print(self.image_size)\n",
    "#         print(\"steps\")\n",
    "#         print(self.steps)\n",
    "#         print(\"feature_maps\")\n",
    "#         print(self.feature_maps)\n",
    "        \n",
    "        \n",
    "    def forward(self):\n",
    "        anchors = []\n",
    "        for k, f in enumerate(self.feature_maps):\n",
    "#             print(f)\n",
    "            min_sizes = self.min_sizes[k]\n",
    "            for i, j in product(range(f[0]), range(f[1])):\n",
    "                for min_size in min_sizes:\n",
    "                    s_kx = min_size / self.image_size[1]\n",
    "                    s_ky = min_size / self.image_size[0]\n",
    "                    dense_cx = [x * self.steps[k] / self.image_size[1] for x in [j + 0.5]]\n",
    "                    dense_cy = [y * self.steps[k] / self.image_size[0] for y in [i + 0.5]]\n",
    "#                     print(\"dddddddddddddddddd\")\n",
    "#                     print(s_kx)\n",
    "#                     print(s_ky)\n",
    "#                     print(dense_cx)\n",
    "#                     print(dense_cy)\n",
    "                    for cy, cx in product(dense_cy, dense_cx):\n",
    "#                         print(\"xxxxxxxxxxxxxxxxxxxxxx\")\n",
    "#                         print(cy)\n",
    "#                         print(cx)\n",
    "                        anchors += [cx, cy, s_kx, s_ky]\n",
    "#                         print(anchors)\n",
    "        \n",
    "#         print(\"len anchors\")\n",
    "#         print(len(anchors))\n",
    "        \n",
    "        # back to torch land\n",
    "        output = torch.Tensor(anchors).view(-1, 4)\n",
    "#         print('pppppppppppppp')\n",
    "#         print(output.shape)\n",
    "        if self.clip:\n",
    "            output.clamp_(max=1, min=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "559e6082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pb = priorbox = PriorBox(cfg, image_size=(img_dim, img_dim)) #바운딩박스 만드는 코드\n",
    "# pb.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e49bf3a",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "843fd0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0b649f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = timm.models.convnext_large(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc6afcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1341b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models._utils as _utils\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def conv_bn(inp, oup, stride = 1, leaky = 0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.LeakyReLU(negative_slope=leaky, inplace=True)\n",
    "    )\n",
    "\n",
    "def conv_bn_no_relu(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "    )\n",
    "\n",
    "def conv_bn1X1(inp, oup, stride, leaky=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, stride, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.LeakyReLU(negative_slope=leaky, inplace=True)\n",
    "    )\n",
    "\n",
    "def conv_dw(inp, oup, stride, leaky=0.1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "        nn.BatchNorm2d(inp),\n",
    "        nn.LeakyReLU(negative_slope= leaky,inplace=True),\n",
    "\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.LeakyReLU(negative_slope= leaky,inplace=True),\n",
    "    )\n",
    "\n",
    "class SSH(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(SSH, self).__init__()\n",
    "        assert out_channel % 4 == 0\n",
    "        leaky = 0\n",
    "        if (out_channel <= 64):\n",
    "            leaky = 0.1\n",
    "        self.conv3X3 = conv_bn_no_relu(in_channel, out_channel//2, stride=1)\n",
    "\n",
    "        self.conv5X5_1 = conv_bn(in_channel, out_channel//4, stride=1, leaky = leaky)\n",
    "        self.conv5X5_2 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1)\n",
    "\n",
    "        self.conv7X7_2 = conv_bn(out_channel//4, out_channel//4, stride=1, leaky = leaky)\n",
    "        self.conv7x7_3 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        conv3X3 = self.conv3X3(input)\n",
    "\n",
    "        conv5X5_1 = self.conv5X5_1(input)\n",
    "        conv5X5 = self.conv5X5_2(conv5X5_1)\n",
    "\n",
    "        conv7X7_2 = self.conv7X7_2(conv5X5_1)\n",
    "        conv7X7 = self.conv7x7_3(conv7X7_2)\n",
    "\n",
    "        out = torch.cat([conv3X3, conv5X5, conv7X7], dim=1)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self,in_channels_list,out_channels):\n",
    "        super(FPN,self).__init__()\n",
    "        leaky = 0\n",
    "        if (out_channels <= 64):\n",
    "            leaky = 0.1\n",
    "        self.output1 = conv_bn1X1(in_channels_list[0], out_channels, stride = 1, leaky = leaky)\n",
    "        self.output2 = conv_bn1X1(in_channels_list[1], out_channels, stride = 1, leaky = leaky)\n",
    "        self.output3 = conv_bn1X1(in_channels_list[2], out_channels, stride = 1, leaky = leaky)\n",
    "\n",
    "        self.merge1 = conv_bn(out_channels, out_channels, leaky = leaky)\n",
    "        self.merge2 = conv_bn(out_channels, out_channels, leaky = leaky)\n",
    "\n",
    "    def forward(self, input1, input2, input3):\n",
    "        # names = list(input.keys())\n",
    "#         print(\"input_size\")\n",
    "#         print(input.shape)\n",
    "#         input = list(input.values())\n",
    "\n",
    "        output1 = self.output1(input1)\n",
    "        output2 = self.output2(input2)\n",
    "        output3 = self.output3(input3)\n",
    "\n",
    "        up3 = F.interpolate(output3, size=[output2.size(2), output2.size(3)], mode=\"nearest\")\n",
    "        output2 = output2 + up3\n",
    "        output2 = self.merge2(output2)\n",
    "\n",
    "        up2 = F.interpolate(output2, size=[output1.size(2), output1.size(3)], mode=\"nearest\")\n",
    "        output1 = output1 + up2\n",
    "        output1 = self.merge1(output1)\n",
    "\n",
    "        out = [output1, output2, output3]\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class MobileNetV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MobileNetV1, self).__init__()\n",
    "        self.stage1 = nn.Sequential(\n",
    "            conv_bn(3, 8, 2, leaky = 0.1),    # 3\n",
    "            conv_dw(8, 16, 1),   # 7\n",
    "            conv_dw(16, 32, 2),  # 11\n",
    "            conv_dw(32, 32, 1),  # 19\n",
    "            conv_dw(32, 64, 2),  # 27\n",
    "            conv_dw(64, 64, 1),  # 43\n",
    "        )\n",
    "        self.stage2 = nn.Sequential(\n",
    "            conv_dw(64, 128, 2),  # 43 + 16 = 59\n",
    "            conv_dw(128, 128, 1), # 59 + 32 = 91\n",
    "            conv_dw(128, 128, 1), # 91 + 32 = 123\n",
    "            conv_dw(128, 128, 1), # 123 + 32 = 155\n",
    "            conv_dw(128, 128, 1), # 155 + 32 = 187\n",
    "            conv_dw(128, 128, 1), # 187 + 32 = 219\n",
    "        )\n",
    "        self.stage3 = nn.Sequential(\n",
    "            conv_dw(128, 256, 2), # 219 +3 2 = 241\n",
    "            conv_dw(256, 256, 1), # 241 + 64 = 301\n",
    "        )\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(256, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.avg(x)\n",
    "        # x = self.model(x)\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9baadbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnet = MobileNetV1()\n",
    "# print(mnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3d3a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models.detection.backbone_utils as backbone_utils\n",
    "import torchvision.models._utils as _utils\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "# from models.net import MobileNetV1 as MobileNetV1\n",
    "# from models.net import FPN as FPN\n",
    "# from models.net import SSH as SSH\n",
    "\n",
    "\n",
    "\n",
    "class ClassHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(ClassHead,self).__init__()\n",
    "        self.num_anchors = num_anchors\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,self.num_anchors*2,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "#         print('ClassHead')\n",
    "#         print(x.shape)\n",
    "        out = self.conv1x1(x)\n",
    "#         print(out.shape)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "#         print(out.shape)\n",
    "        \n",
    "        return out.view(out.shape[0], -1, 2)\n",
    "\n",
    "class BboxHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(BboxHead,self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,num_anchors*4,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "#         print('BboxHead')\n",
    "#         print(x.shape)\n",
    "        out = self.conv1x1(x)\n",
    "#         print(out.shape)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "#         print(out.shape)\n",
    "        \n",
    "        return out.view(out.shape[0], -1, 4)\n",
    "\n",
    "class LandmarkHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(LandmarkHead,self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,num_anchors*10,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "#         print('Landmarkhead')\n",
    "#         print(x.shape)\n",
    "        out = self.conv1x1(x)\n",
    "#         print(out.shape)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "#         print(out.shape)\n",
    "\n",
    "        return out.view(out.shape[0], -1, 10)\n",
    "\n",
    "class RetinaFace(nn.Module):\n",
    "    def __init__(self, cfg = None, phase = 'train'):\n",
    "        \"\"\"\n",
    "        :param cfg:  Network related settings.\n",
    "        :param phase: train or test.\n",
    "        \"\"\"\n",
    "        super(RetinaFace,self).__init__()\n",
    "        self.phase = phase\n",
    "        backbone = None\n",
    "        if cfg['name'] == 'mobilenet0.25':\n",
    "            backbone = MobileNetV1()\n",
    "            if cfg['pretrain']:\n",
    "                checkpoint = torch.load(\"./weights/mobilenetV1X0.25_pretrain.tar\", map_location=torch.device('cuda'))\n",
    "                from collections import OrderedDict\n",
    "                new_state_dict = OrderedDict()\n",
    "                for k, v in checkpoint['state_dict'].items():\n",
    "                    name = k[7:]  # remove module.\n",
    "                    new_state_dict[name] = v\n",
    "                # load params\n",
    "                backbone.load_state_dict(new_state_dict)\n",
    "        elif cfg['name'] == 'Resnet50':\n",
    "            import torchvision.models as models\n",
    "            backbone = models.resnet50(pretrained=cfg['pretrain'])\n",
    "        elif cfg['name'] == 'convnext_large':\n",
    "            import timm\n",
    "            backbone = timm.models.convnext_tiny(pretrained=True)\n",
    "        \n",
    "        self.body = _utils.IntermediateLayerGetter(backbone, cfg['return_layers'])\n",
    "        # 가정: body['head']가 수정하고자 하는 Sequential 객체\n",
    "        head_layers = list(self.body['head'].children())\n",
    "\n",
    "        # 유지하고 싶은 레이어 이름 목록\n",
    "        keep_layers = ['global_pool', 'norm']\n",
    "\n",
    "        # 해당 레이어만 유지\n",
    "        head_layers = [layer for layer, name in zip(head_layers, keep_layers) if name in keep_layers]\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        head_layers.append(self.leaky_relu)\n",
    "        # 다시 Sequential로 변환\n",
    "        self.body['head'] = nn.Sequential(*head_layers)\n",
    "        \n",
    "        \n",
    "        self.max_pool_1 = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.transposed_conv_1 = nn.ConvTranspose2d(in_channels=96, out_channels=96, kernel_size=105, stride=1, padding=0)\n",
    "        self.transposed_conv_2 = nn.ConvTranspose2d(in_channels=768, out_channels=768, kernel_size=53, stride=1, padding=0)\n",
    "        self.transposed_conv_3 = nn.ConvTranspose2d(in_channels=768, out_channels=768, kernel_size=27, stride=1, padding=0)\n",
    "        \n",
    "#         print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1\")\n",
    "#         print(self.body)\n",
    "        \n",
    "#         self.cnext_last_layer = torch.nn.Conv2d(768, 256, kernel_size=1)\n",
    "#         self.body = del self.body.ConvNeXtStage[2]\n",
    "        \n",
    "        #         print(\"backbone\")\n",
    "#         print(backbone)\n",
    "#         print(\"body\")\n",
    "#         print(self.body)\n",
    "        \n",
    "    \n",
    "    \n",
    "        in_channels_stage2 = cfg['in_channel']\n",
    "        in_channels_list = [\n",
    "            96,\n",
    "            768,\n",
    "            768\n",
    "        ]\n",
    "        out_channels = cfg['out_channel']\n",
    "        self.fpn = FPN(in_channels_list,out_channels) #Unet구조로 upsampling\n",
    "        self.ssh1 = SSH(out_channels, out_channels) # layer을 2개쌓고 activation function을 줘서 세부정보를 포착\n",
    "        self.ssh2 = SSH(out_channels, out_channels)\n",
    "        self.ssh3 = SSH(out_channels, out_channels)\n",
    "\n",
    "        self.ClassHead = self._make_class_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
    "        self.BboxHead = self._make_bbox_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
    "        self.LandmarkHead = self._make_landmark_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _make_class_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        classhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            classhead.append(ClassHead(inchannels,anchor_num))\n",
    "        return classhead\n",
    "    \n",
    "    def _make_bbox_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        bboxhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            bboxhead.append(BboxHead(inchannels,anchor_num))\n",
    "        return bboxhead\n",
    "\n",
    "    def _make_landmark_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        landmarkhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            landmarkhead.append(LandmarkHead(inchannels,anchor_num))\n",
    "        return landmarkhead\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        out = self.body(inputs)\n",
    "#         print(out.)\n",
    "        \n",
    "#         print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "#         for key, value in out.items():\n",
    "#             print(\"Key:\", key)\n",
    "#             print(\"Value:\", value.shape)\n",
    "        \n",
    "#         for k, v in out.items():  # IntermediateLayerGetter returns a dict\n",
    "#             if v.shape[1] == 768:  # Apply the conv layer to the output with 768 channels\n",
    "#                 out[k] = self.cnext_last_layer(v)\n",
    "        \n",
    "        out[1] = self.max_pool_1(out[1])\n",
    "        out[1] = self.transposed_conv_1(out[1])\n",
    "    \n",
    "        out[2] = self.max_pool_1(out[2])\n",
    "        out[2] = self.transposed_conv_2(out[2])\n",
    "        \n",
    "        out[3] = self.transposed_conv_3(out[3])\n",
    "        \n",
    "        \n",
    "        # FPN\n",
    "        fpn = self.fpn(out[1], out[2], out[3])\n",
    "        \n",
    "        # SSH\n",
    "        feature1 = self.ssh1(fpn[0])\n",
    "        feature2 = self.ssh2(fpn[1])\n",
    "        feature3 = self.ssh3(fpn[2])\n",
    "        \n",
    "#         print(\"fpn\")\n",
    "#         print(fpn[0].shape)\n",
    "#         print(fpn[1].shape)\n",
    "#         print(fpn[2].shape)\n",
    "        \n",
    "#         print(\"ssh\")\n",
    "#         print(feature1.shape)\n",
    "#         print(feature2.shape)\n",
    "#         print(feature3.shape)\n",
    "        \n",
    "        features = [feature1, feature2, feature3]\n",
    "        \n",
    "#         for i, feature in enumerate(features):\n",
    "#             print(f\"feature{i+1} size: {feature.size()}\")\n",
    "            \n",
    "        bbox_regressions = torch.cat([self.BboxHead[i](feature) for i, feature in enumerate(features)], dim=1)\n",
    "        classifications = torch.cat([self.ClassHead[i](feature) for i, feature in enumerate(features)],dim=1)\n",
    "        ldm_regressions = torch.cat([self.LandmarkHead[i](feature) for i, feature in enumerate(features)], dim=1)\n",
    "#         print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "#         print(bbox_regressions.shape)\n",
    "#         print(classifications.shape)\n",
    "#         print(ldm_regressions.shape)\n",
    "        \n",
    "        \n",
    "        if self.phase == 'train':\n",
    "            output = (bbox_regressions, classifications, ldm_regressions)\n",
    "        else:\n",
    "            output = (bbox_regressions, F.softmax(classifications, dim=-1), ldm_regressions)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435b7fa3",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7a96f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "from tqdm import tqdm\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    'training_dataset' : '/data/dhk/face/face_detecte/widerface/train/label.txt',\n",
    "    'network' : 'convnext_large',\n",
    "    'num_workers' :  0,\n",
    "    'lr' : 1e-3,\n",
    "    'momentum' : 0.9,\n",
    "    'resume_net' : None,\n",
    "    'resume_epoch' : 0,\n",
    "    'weight_decay' : 5e-4,\n",
    "    'gamma' : 0.1,\n",
    "    'save_folder' : './weights/',\n",
    "    \n",
    "#     #여기서부터 학습\n",
    "#     'resume_net' : \"./weights/convnext_large_epoch_30.pth\",\n",
    "#     'resume_epoch' : 30,\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e9a3fbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing net...\n",
      "RetinaFace(\n",
      "  (body): IntermediateLayerGetter(\n",
      "    (stem): Sequential(\n",
      "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "    (stages): Sequential(\n",
      "      (0): ConvNeXtStage(\n",
      "        (downsample): Identity()\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (3): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (4): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (5): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (6): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (7): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (8): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate=none)\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_pre): Identity()\n",
      "    (head): Sequential(\n",
      "      (0): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n",
      "      (1): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  (max_pool_1): AdaptiveMaxPool2d(output_size=(1, 1))\n",
      "  (transposed_conv_1): ConvTranspose2d(96, 96, kernel_size=(105, 105), stride=(1, 1))\n",
      "  (transposed_conv_2): ConvTranspose2d(768, 768, kernel_size=(53, 53), stride=(1, 1))\n",
      "  (transposed_conv_3): ConvTranspose2d(768, 768, kernel_size=(27, 27), stride=(1, 1))\n",
      "  (fpn): FPN(\n",
      "    (output1): Sequential(\n",
      "      (0): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0, inplace=True)\n",
      "    )\n",
      "    (output2): Sequential(\n",
      "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0, inplace=True)\n",
      "    )\n",
      "    (output3): Sequential(\n",
      "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0, inplace=True)\n",
      "    )\n",
      "    (merge1): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0, inplace=True)\n",
      "    )\n",
      "    (merge2): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (ssh1): SSH(\n",
      "    (conv3X3): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv5X5_1): Sequential(\n",
      "      (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0, inplace=True)\n",
      "    )\n",
      "    (conv5X5_2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv7X7_2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0, inplace=True)\n",
      "    )\n",
      "    (conv7x7_3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (ssh2): SSH(\n",
      "    (conv3X3): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv5X5_1): Sequential(\n",
      "      (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0, inplace=True)\n",
      "    )\n",
      "    (conv5X5_2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv7X7_2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0, inplace=True)\n",
      "    )\n",
      "    (conv7x7_3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (ssh3): SSH(\n",
      "    (conv3X3): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv5X5_1): Sequential(\n",
      "      (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0, inplace=True)\n",
      "    )\n",
      "    (conv5X5_2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv7X7_2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0, inplace=True)\n",
      "    )\n",
      "    (conv7x7_3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (ClassHead): ModuleList(\n",
      "    (0): ClassHead(\n",
      "      (conv1x1): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): ClassHead(\n",
      "      (conv1x1): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (2): ClassHead(\n",
      "      (conv1x1): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (BboxHead): ModuleList(\n",
      "    (0): BboxHead(\n",
      "      (conv1x1): Conv2d(256, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): BboxHead(\n",
      "      (conv1x1): Conv2d(256, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (2): BboxHead(\n",
      "      (conv1x1): Conv2d(256, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (LandmarkHead): ModuleList(\n",
      "    (0): LandmarkHead(\n",
      "      (conv1x1): Conv2d(256, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): LandmarkHead(\n",
      "      (conv1x1): Conv2d(256, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (2): LandmarkHead(\n",
      "      (conv1x1): Conv2d(256, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Loading resume network...\n",
      "Loading Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                         | 1/300580 [00:54<4573:41:41, 54.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 1/4294 || Iter: 128821/429400 || Loc: 2.3042 Cla: 2.6143 Landm: 6.4057 || LR: 0.00100000 || Batchtime: 41.2181 s || ETA: 143 days, 9:29:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                         | 2/300580 [00:55<1930:41:00, 23.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 2/4294 || Iter: 128822/429400 || Loc: 1.1234 Cla: 1.6305 Landm: 2.8694 || LR: 0.00100000 || Batchtime: 0.8007 s || ETA: 2 days, 18:51:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                         | 3/300580 [00:56<1091:07:33, 13.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 3/4294 || Iter: 128823/429400 || Loc: 1.2139 Cla: 1.2735 Landm: 4.2410 || LR: 0.00100000 || Batchtime: 0.9391 s || ETA: 3 days, 6:24:43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 4/300580 [00:57<696:28:18,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 4/4294 || Iter: 128824/429400 || Loc: 2.1467 Cla: 2.4438 Landm: 7.7295 || LR: 0.00100000 || Batchtime: 0.9332 s || ETA: 3 days, 5:55:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 5/300580 [00:59<478:21:03,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 5/4294 || Iter: 128825/429400 || Loc: 0.9368 Cla: 1.1698 Landm: 2.0097 || LR: 0.00100000 || Batchtime: 0.9358 s || ETA: 3 days, 6:08:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 6/300580 [01:00<347:00:29,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 6/4294 || Iter: 128826/429400 || Loc: 1.7693 Cla: 1.4351 Landm: 2.8791 || LR: 0.00100000 || Batchtime: 0.9398 s || ETA: 3 days, 6:28:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 7/300580 [01:01<263:31:22,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 7/4294 || Iter: 128827/429400 || Loc: 0.3714 Cla: 0.8116 Landm: 1.1563 || LR: 0.00100000 || Batchtime: 0.9348 s || ETA: 3 days, 6:02:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 8/300580 [01:02<208:49:31,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 8/4294 || Iter: 128828/429400 || Loc: 2.4281 Cla: 2.8353 Landm: 14.2419 || LR: 0.00100000 || Batchtime: 0.9368 s || ETA: 3 days, 6:13:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 9/300580 [01:03<172:15:12,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 9/4294 || Iter: 128829/429400 || Loc: 1.8371 Cla: 1.7529 Landm: 5.2811 || LR: 0.00100000 || Batchtime: 0.9374 s || ETA: 3 days, 6:16:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                         | 10/300580 [01:04<147:33:34,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 10/4294 || Iter: 128830/429400 || Loc: 3.2227 Cla: 3.1823 Landm: 9.5236 || LR: 0.00100000 || Batchtime: 0.9423 s || ETA: 3 days, 6:40:38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                         | 11/300580 [01:05<130:06:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 11/4294 || Iter: 128831/429400 || Loc: 0.6674 Cla: 0.6648 Landm: 3.5038 || LR: 0.00100000 || Batchtime: 0.9216 s || ETA: 3 days, 4:56:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                         | 12/300580 [01:06<117:52:14,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 12/4294 || Iter: 128832/429400 || Loc: 1.4351 Cla: 1.7042 Landm: 3.9756 || LR: 0.00100000 || Batchtime: 0.9139 s || ETA: 3 days, 4:18:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                         | 13/300580 [01:07<109:53:11,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 13/4294 || Iter: 128833/429400 || Loc: 1.7910 Cla: 1.6166 Landm: 6.4922 || LR: 0.00100000 || Batchtime: 0.9350 s || ETA: 3 days, 6:04:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                         | 14/300580 [01:08<104:28:26,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 14/4294 || Iter: 128834/429400 || Loc: 0.9824 Cla: 0.9689 Landm: 0.8449 || LR: 0.00100000 || Batchtime: 0.9411 s || ETA: 3 days, 6:34:38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                         | 15/300580 [01:10<100:48:39,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 15/4294 || Iter: 128835/429400 || Loc: 1.0259 Cla: 1.4422 Landm: 3.9362 || LR: 0.00100000 || Batchtime: 0.9421 s || ETA: 3 days, 6:39:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 16/300580 [01:11<98:00:22,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 16/4294 || Iter: 128836/429400 || Loc: 1.7758 Cla: 2.4448 Landm: 5.3454 || LR: 0.00100000 || Batchtime: 0.9357 s || ETA: 3 days, 6:07:21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 17/300580 [01:12<96:16:24,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 17/4294 || Iter: 128837/429400 || Loc: 1.8962 Cla: 2.1159 Landm: 11.4943 || LR: 0.00100000 || Batchtime: 0.9402 s || ETA: 3 days, 6:29:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 18/300580 [01:13<94:55:59,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 18/4294 || Iter: 128838/429400 || Loc: 0.7696 Cla: 1.3196 Landm: 6.1080 || LR: 0.00100000 || Batchtime: 0.9362 s || ETA: 3 days, 6:09:54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 19/300580 [01:14<93:56:05,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 19/4294 || Iter: 128839/429400 || Loc: 2.4152 Cla: 2.3934 Landm: 7.2540 || LR: 0.00100000 || Batchtime: 0.9348 s || ETA: 3 days, 6:02:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 20/300580 [01:15<93:16:20,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 20/4294 || Iter: 128840/429400 || Loc: 1.6214 Cla: 2.1546 Landm: 6.0218 || LR: 0.00100000 || Batchtime: 0.9352 s || ETA: 3 days, 6:04:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 21/300580 [01:16<92:57:58,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 21/4294 || Iter: 128841/429400 || Loc: 2.1235 Cla: 2.3811 Landm: 1.0993 || LR: 0.00100000 || Batchtime: 0.9439 s || ETA: 3 days, 6:48:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 22/300580 [01:17<92:38:46,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 22/4294 || Iter: 128842/429400 || Loc: 0.4241 Cla: 0.7360 Landm: 0.8467 || LR: 0.00100000 || Batchtime: 0.9385 s || ETA: 3 days, 6:21:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 23/300580 [01:18<92:18:02,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 23/4294 || Iter: 128843/429400 || Loc: 0.4441 Cla: 0.6940 Landm: 3.7414 || LR: 0.00100000 || Batchtime: 0.9352 s || ETA: 3 days, 6:04:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 24/300580 [01:19<92:06:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 24/4294 || Iter: 128844/429400 || Loc: 0.9251 Cla: 1.4658 Landm: 3.8233 || LR: 0.00100000 || Batchtime: 0.9357 s || ETA: 3 days, 6:07:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 25/300580 [01:21<92:01:11,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 25/4294 || Iter: 128845/429400 || Loc: 1.4391 Cla: 1.6673 Landm: 7.0280 || LR: 0.00100000 || Batchtime: 0.9384 s || ETA: 3 days, 6:20:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 26/300580 [01:22<91:31:33,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 26/4294 || Iter: 128846/429400 || Loc: 0.6804 Cla: 0.8311 Landm: 2.1367 || LR: 0.00100000 || Batchtime: 0.9193 s || ETA: 3 days, 4:45:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 27/300580 [01:23<91:58:05,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 27/4294 || Iter: 128847/429400 || Loc: 1.4481 Cla: 1.9116 Landm: 2.7483 || LR: 0.00100000 || Batchtime: 0.9503 s || ETA: 3 days, 7:20:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 28/300580 [01:24<91:28:55,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 28/4294 || Iter: 128848/429400 || Loc: 0.7083 Cla: 0.9838 Landm: 2.2742 || LR: 0.00100000 || Batchtime: 0.9205 s || ETA: 3 days, 4:51:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 29/300580 [01:25<91:36:04,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 29/4294 || Iter: 128849/429400 || Loc: 1.2144 Cla: 1.1069 Landm: 2.2815 || LR: 0.00100000 || Batchtime: 0.9380 s || ETA: 3 days, 6:18:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 30/300580 [01:26<91:17:22,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 30/4294 || Iter: 128850/429400 || Loc: 3.7664 Cla: 2.9580 Landm: 13.0194 || LR: 0.00100000 || Batchtime: 0.9235 s || ETA: 3 days, 5:05:46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 31/300580 [01:27<91:34:28,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 31/4294 || Iter: 128851/429400 || Loc: 2.7841 Cla: 2.5545 Landm: 8.2937 || LR: 0.00100000 || Batchtime: 0.9411 s || ETA: 3 days, 6:34:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 32/300580 [01:28<91:32:29,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 32/4294 || Iter: 128852/429400 || Loc: 0.5250 Cla: 0.9921 Landm: 2.9204 || LR: 0.00100000 || Batchtime: 0.9344 s || ETA: 3 days, 6:00:23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 33/300580 [01:29<91:46:19,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 33/4294 || Iter: 128853/429400 || Loc: 1.5507 Cla: 1.8020 Landm: 5.0400 || LR: 0.00100000 || Batchtime: 0.9451 s || ETA: 3 days, 6:54:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 34/300580 [01:30<91:50:05,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 34/4294 || Iter: 128854/429400 || Loc: 1.5599 Cla: 1.9446 Landm: 5.0650 || LR: 0.00100000 || Batchtime: 0.9377 s || ETA: 3 days, 6:17:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 35/300580 [01:31<91:50:15,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 35/4294 || Iter: 128855/429400 || Loc: 1.5798 Cla: 2.1716 Landm: 8.4387 || LR: 0.00100000 || Batchtime: 0.9384 s || ETA: 3 days, 6:20:43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 36/300580 [01:33<91:53:38,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 36/4294 || Iter: 128856/429400 || Loc: 3.3307 Cla: 2.9028 Landm: 11.0731 || LR: 0.00100000 || Batchtime: 0.9389 s || ETA: 3 days, 6:22:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 37/300580 [01:34<91:59:07,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 37/4294 || Iter: 128857/429400 || Loc: 3.3168 Cla: 3.1650 Landm: 15.5418 || LR: 0.00100000 || Batchtime: 0.9410 s || ETA: 3 days, 6:33:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 38/300580 [01:35<92:03:20,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 38/4294 || Iter: 128858/429400 || Loc: 0.7185 Cla: 1.3126 Landm: 2.5378 || LR: 0.00100000 || Batchtime: 0.9420 s || ETA: 3 days, 6:38:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 39/300580 [01:36<92:10:10,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 39/4294 || Iter: 128859/429400 || Loc: 0.5231 Cla: 0.7099 Landm: 4.7102 || LR: 0.00100000 || Batchtime: 0.9437 s || ETA: 3 days, 6:46:51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 40/300580 [01:37<92:02:07,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 40/4294 || Iter: 128860/429400 || Loc: 1.3019 Cla: 2.2624 Landm: 6.4614 || LR: 0.00100000 || Batchtime: 0.9372 s || ETA: 3 days, 6:14:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 41/300580 [01:38<92:02:58,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 41/4294 || Iter: 128861/429400 || Loc: 2.0326 Cla: 2.1800 Landm: 10.6178 || LR: 0.00100000 || Batchtime: 0.9427 s || ETA: 3 days, 6:41:46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 42/300580 [01:39<91:57:42,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 42/4294 || Iter: 128862/429400 || Loc: 2.5018 Cla: 2.2590 Landm: 7.4482 || LR: 0.00100000 || Batchtime: 0.9387 s || ETA: 3 days, 6:22:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 43/300580 [01:40<92:02:11,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 43/4294 || Iter: 128863/429400 || Loc: 2.8020 Cla: 2.4779 Landm: 12.2708 || LR: 0.00100000 || Batchtime: 0.9442 s || ETA: 3 days, 6:49:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 44/300580 [01:41<92:04:40,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31/100 || Epochiter: 44/4294 || Iter: 128864/429400 || Loc: 2.7526 Cla: 2.6406 Landm: 12.5519 || LR: 0.00100000 || Batchtime: 0.9429 s || ETA: 3 days, 6:42:43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                         | 44/300580 [01:42<194:40:17,  2.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 160\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lr\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 132\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m loss \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloc_weight\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m loss_l \u001b[38;5;241m+\u001b[39m loss_c \u001b[38;5;241m+\u001b[39m loss_landm\n\u001b[1;32m    131\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 132\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m load_t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    134\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/torch/optim/sgd.py:146\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m             momentum_buffer_list\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum_buffer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 146\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/torch/optim/sgd.py:197\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 197\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m     \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dhk/lib/python3.8/site-packages/torch/optim/sgd.py:241\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    238\u001b[0m         d_p \u001b[38;5;241m=\u001b[39m buf\n\u001b[1;32m    240\u001b[0m alpha \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;28;01mif\u001b[39;00m maximize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39mlr\n\u001b[0;32m--> 241\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.save_folder):\n",
    "    os.mkdir(args.save_folder)\n",
    "cfg = None\n",
    "if args.network == \"mobile0.25\":\n",
    "    cfg = cfg_mnet\n",
    "elif args.network == \"resnet50\":\n",
    "    cfg = cfg_re50\n",
    "elif args.network == \"convnext_large\":\n",
    "    cfg = cfg_convnext\n",
    "    \n",
    "rgb_mean = (104, 117, 123) # bgr order\n",
    "num_classes = 2\n",
    "img_dim = cfg['image_size']\n",
    "num_gpu = cfg['ngpu']\n",
    "batch_size = cfg['batch_size']\n",
    "max_epoch = cfg['epoch']\n",
    "gpu_train = cfg['gpu_train']\n",
    "\n",
    "num_workers = args.num_workers\n",
    "momentum = args.momentum\n",
    "weight_decay = args.weight_decay\n",
    "initial_lr = args.lr\n",
    "gamma = args.gamma\n",
    "training_dataset = args.training_dataset\n",
    "save_folder = args.save_folder\n",
    "\n",
    "net = RetinaFace(cfg=cfg)\n",
    "print(\"Printing net...\")\n",
    "print(net)\n",
    "\n",
    "if args.resume_net is not None:\n",
    "    print('Loading resume network...')\n",
    "    state_dict = torch.load(args.resume_net, map_location=torch.device('cpu'))\n",
    "    # create new OrderedDict that does not contain `module.`\n",
    "#     from collections import OrderedDict\n",
    "#     new_state_dict = OrderedDict()\n",
    "#     for k, v in state_dict.items():\n",
    "#         head = k[:7]\n",
    "#         if head == 'module.':\n",
    "#             name = k[7:] # remove `module.`\n",
    "#         else:\n",
    "#             name = k\n",
    "#         new_state_dict[name] = v\n",
    "    net.load_state_dict(state_dict)\n",
    "\n",
    "if num_gpu > 1 and gpu_train:\n",
    "    net = torch.nn.DataParallel(net).cuda()\n",
    "else:\n",
    "    net = net.cuda()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "# # 현재 할당된 메모리 양(바이트 단위)\n",
    "# mem_bytes = torch.cuda.memory_allocated()\n",
    "\n",
    "# # 기가바이트 단위로 변환\n",
    "# mem_gb = mem_bytes / (1024 ** 3)\n",
    "# print(f\"Currently allocated memory: {mem_gb} GB\")\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = MultiBoxLoss(num_classes, 0.35, True, 0, True, 7, 0.35, False) #객체탐지 loss\n",
    "\n",
    "priorbox = PriorBox(cfg, image_size=(img_dim, img_dim)) #바운딩박스 만드는 코드\n",
    "# print(\"priorbox\")\n",
    "# print(priorbox)\n",
    "\n",
    "with torch.no_grad():\n",
    "    priors = priorbox.forward()\n",
    "    priors = priors.cuda()\n",
    "\n",
    "def train():\n",
    "    net.train()\n",
    "    epoch = 0 + args.resume_epoch\n",
    "    print('Loading Dataset...')\n",
    "\n",
    "    dataset = WiderFaceDetection(training_dataset,preproc(img_dim, rgb_mean))\n",
    "#     print(dataset[0])\n",
    "    \n",
    "    epoch_size = math.ceil(len(dataset) / batch_size)\n",
    "    max_iter = max_epoch * epoch_size\n",
    "\n",
    "    stepvalues = (cfg['decay1'] * epoch_size, cfg['decay2'] * epoch_size)\n",
    "    step_index = 0\n",
    "\n",
    "    if args.resume_epoch > 0:\n",
    "        start_iter = args.resume_epoch * epoch_size\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    for iteration in tqdm(range(start_iter, max_iter)):\n",
    "#         torch.cuda.empty_cache() \n",
    "        if iteration % epoch_size == 0:\n",
    "            # create batch iterator\n",
    "            batch_iterator = iter(data.DataLoader(dataset, batch_size, shuffle=True, num_workers=num_workers, collate_fn=detection_collate))\n",
    "            if (epoch % 10 == 0 and epoch > 0) or (epoch % 5 == 0 and epoch > cfg['decay1']):\n",
    "                torch.save(net.state_dict(), save_folder + cfg['name']+ '_epoch_' + str(epoch) + '.pth')\n",
    "            epoch += 1\n",
    "\n",
    "        load_t0 = time.time()\n",
    "        if iteration in stepvalues:\n",
    "            step_index += 1\n",
    "        lr = adjust_learning_rate(optimizer, gamma, epoch, step_index, iteration, epoch_size)\n",
    "\n",
    "        # load train data\n",
    "        images, targets = next(batch_iterator)\n",
    "#         print(images.shape)\n",
    "#         print(target[0])\n",
    "        \n",
    "        images = images.cuda()\n",
    "        targets = [anno.cuda() for anno in targets]\n",
    "        \n",
    "#         # 현재 할당된 메모리 양(바이트 단위)\n",
    "#         mem_bytes = torch.cuda.memory_allocated()\n",
    "\n",
    "#         # 기가바이트 단위로 변환\n",
    "#         mem_gb = mem_bytes / (1024 ** 3)\n",
    "#         print(f\"Currently allocated memory: {mem_gb} GB\")\n",
    "\n",
    "        # forward\n",
    "        out = net(images)\n",
    "#         for i, output in enumerate(out):\n",
    "#             print(f\"Shape of output {i}: {output.shape}\")\n",
    "            \n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss_l, loss_c, loss_landm = criterion(out, priors, targets)\n",
    "        loss = cfg['loc_weight'] * loss_l + loss_c + loss_landm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        load_t1 = time.time()\n",
    "        torch.cuda.empty_cache()\n",
    "        batch_time = load_t1 - load_t0\n",
    "        eta = int(batch_time * (max_iter - iteration))\n",
    "        print('Epoch:{}/{} || Epochiter: {}/{} || Iter: {}/{} || Loc: {:.4f} Cla: {:.4f} Landm: {:.4f} || LR: {:.8f} || Batchtime: {:.4f} s || ETA: {}'\n",
    "              .format(epoch, max_epoch, (iteration % epoch_size) + 1,\n",
    "              epoch_size, iteration + 1, max_iter, loss_l.item(), loss_c.item(), loss_landm.item(), lr, batch_time, str(datetime.timedelta(seconds=eta))))\n",
    "\n",
    "    torch.save(net.state_dict(), save_folder + cfg['name'] + '_Final.pth')\n",
    "    # torch.save(net.state_dict(), save_folder + 'Final_Retinaface.pth')\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, gamma, epoch, step_index, iteration, epoch_size):\n",
    "    \"\"\"Sets the learning rate\n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    warmup_epoch = -1\n",
    "    if epoch <= warmup_epoch:\n",
    "        lr = 1e-6 + (initial_lr-1e-6) * iteration / (epoch_size * warmup_epoch)\n",
    "    else:\n",
    "        lr = initial_lr * (gamma ** (step_index))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c0f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fffffffffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014dba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_intermediate_layers(model, layer_names):\n",
    "#     outputs = {}\n",
    "#     for name, module in model.named_modules():\n",
    "#         if name in layer_names:\n",
    "#             outputs[name] = module\n",
    "#     return outputs\n",
    "\n",
    "backbone = timm.models.convnext_tiny(pretrained=True)\n",
    "# print(backbone)\n",
    "# print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "for name, module in backbone.named_children():\n",
    "    print(name)\n",
    " \n",
    "\n",
    "\n",
    "# # for name, module in backbone.named_modules():\n",
    "# #     print(name, module)\n",
    "\n",
    "cfg_convnext = {\n",
    "    'name': 'convnext_large',\n",
    "    'min_sizes': [[16, 32], [64, 128], [256, 512]],\n",
    "    'steps': [8, 16, 32],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': False,\n",
    "    'loc_weight': 2.0,\n",
    "    'gpu_train': True,\n",
    "    'batch_size': 6,\n",
    "    'ngpu': 1,\n",
    "    'epoch': 100,\n",
    "    'decay1': 70,\n",
    "    'decay2': 90,\n",
    "    'image_size': 840,\n",
    "    'pretrain': True,\n",
    "    'return_layers': {'stem': 1, 'stages':2, 'head' : 3},\n",
    "    'in_channel': 256,\n",
    "    'out_channel': 256\n",
    "}\n",
    "\n",
    "body = _utils.IntermediateLayerGetter(backbone, cfg_convnext['return_layers'])\n",
    "\n",
    "# 가정: body['head']가 수정하고자 하는 Sequential 객체\n",
    "head_layers = list(body['head'].children())\n",
    "\n",
    "# 유지하고 싶은 레이어 이름 목록\n",
    "keep_layers = ['global_pool', 'norm']\n",
    "\n",
    "# 해당 레이어만 유지\n",
    "head_layers = [layer for layer, name in zip(head_layers, keep_layers) if name in keep_layers]\n",
    "\n",
    "# 다시 Sequential로 변환\n",
    "body['head'] = nn.Sequential(*head_layers)\n",
    "\n",
    "print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1294ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab972e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_convnext = {\n",
    "    'name': 'convnext_large',\n",
    "    'min_sizes': [[16, 32], [64, 128], [256, 512]],\n",
    "    'steps': [8, 16, 32],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': False,\n",
    "    'loc_weight': 2.0,\n",
    "    'gpu_train': True,\n",
    "    'batch_size': 6,\n",
    "    'ngpu': 1,\n",
    "    'epoch': 100,\n",
    "    'decay1': 70,\n",
    "    'decay2': 90,\n",
    "    'image_size': 840,\n",
    "    'pretrain': True,\n",
    "    'return_layers': {'stem': 1, 'stages': 2, 'head' : 3},\n",
    "    'in_channel': 256,\n",
    "    'out_channel': 256\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d57c3b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from thop import clever_format, profile\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = RetinaFace(cfg=cfg_convnext)\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # from torchvision.models import efficientnet\n",
    "    model = net.to(device)\n",
    "\n",
    "    dummy_input = torch.randn(1, 3, 840, 840).to(device)\n",
    "    flops, params = profile(model.to(device), (dummy_input,), verbose=False)\n",
    "    flops = flops * 2\n",
    "    flops, params = clever_format([flops, params], \"%.3f\") #초당 실행\n",
    "    print('Total GFLOPS: %s' % (flops)) #얼마나 빠르게 동작하는\n",
    "    print('Total params: %s' % (params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f85cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재 할당된 메모리 양(바이트 단위)\n",
    "import torch\n",
    "mem_bytes = torch.cuda.memory_allocated()\n",
    "\n",
    "# 기가바이트 단위로 변환\n",
    "mem_gb = mem_bytes / (1024 ** 3)\n",
    "print(f\"Currently allocated memory: {mem_gb} GB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dhk",
   "language": "python",
   "name": "dhk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
