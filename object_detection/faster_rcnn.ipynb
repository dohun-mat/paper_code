{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.ops import RoIPool\n",
    "\n",
    "# from model.region_proposal_network import RegionProposalNetwork\n",
    "# from model.faster_rcnn import FasterRCNN\n",
    "# from utils import array_tool as at\n",
    "# from utils.config import opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# Default Configs for training\n",
    "# NOTE that, config items could be overwriten by passing argument through command line.\n",
    "# e.g. --voc-data-dir='./data/'\n",
    "\n",
    "class Config:\n",
    "    # data\n",
    "    voc_data_dir = './VOCdevkit/VOC2007/'\n",
    "    min_size = 600  # image resize\n",
    "    max_size = 1000 # image resize\n",
    "    num_workers = 8\n",
    "    test_num_workers = 8\n",
    "\n",
    "    # sigma for l1_smooth_loss\n",
    "    rpn_sigma = 3.\n",
    "    roi_sigma = 1.\n",
    "\n",
    "    # param for optimizer\n",
    "    # 0.0005 in origin paper but 0.0001 in tf-faster-rcnn\n",
    "    weight_decay = 0.0005\n",
    "    lr_decay = 0.1  # 1e-3 -> 1e-4\n",
    "    lr = 1e-3\n",
    "\n",
    "\n",
    "    # visualization\n",
    "    env = 'faster-rcnn'  # visdom env\n",
    "    port = 8097\n",
    "    plot_every = 40  # vis every N iter\n",
    "\n",
    "    # preset\n",
    "    data = 'voc'\n",
    "    pretrained_model = 'vgg16'\n",
    "\n",
    "    # training\n",
    "    epoch = 14\n",
    "\n",
    "\n",
    "    use_adam = False # Use Adam optimizer\n",
    "    use_chainer = False # try match everything as chainer\n",
    "    use_drop = False # use dropout in RoIHead\n",
    "    # debug\n",
    "    debug_file = '/tmp/debugf'\n",
    "\n",
    "    test_num = 10000\n",
    "    # model\n",
    "    load_path = None\n",
    "\n",
    "    caffe_pretrain = False # use caffe pretrained model instead of torchvision\n",
    "    caffe_pretrain_path = 'checkpoints/vgg16_caffe.pth'\n",
    "\n",
    "    def _parse(self, kwargs):\n",
    "        state_dict = self._state_dict()\n",
    "        for k, v in kwargs.items():\n",
    "            if k not in state_dict:\n",
    "                raise ValueError('UnKnown Option: \"--%s\"' % k)\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        print('======user config========')\n",
    "        pprint(self._state_dict())\n",
    "        print('==========end============')\n",
    "\n",
    "    def _state_dict(self):\n",
    "        return {k: getattr(self, k) for k, _ in Config.__dict__.items() \\\n",
    "                if not k.startswith('_')}\n",
    "\n",
    "\n",
    "opt = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def tonumpy(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def totensor(data, cuda = True):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        tensor = torch.from_numpy(data)\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        tensor = data.detach()\n",
    "    if cuda:\n",
    "        tensor = tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "def scalar(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data.reshape(1)[0]\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import six\n",
    "\n",
    "def loc2bbox(src_bbox, loc):\n",
    "    \"\"\"Decode bounding boxes from bounding box offsets and scales.\n",
    "    Given bounding box offsets and scales computed by\n",
    "    :meth:`bbox2loc`, this function decodes the representation to\n",
    "    coordinates in 2D image coordinates.\n",
    "    Given scales and offsets :math:`t_y, t_x, t_h, t_w` and a bounding\n",
    "    box whose center is :math:`(y, x) = p_y, p_x` and size :math:`p_h, p_w`,\n",
    "    the decoded bounding box's center :math:`\\\\hat{g}_y`, :math:`\\\\hat{g}_x`\n",
    "    and size :math:`\\\\hat{g}_h`, :math:`\\\\hat{g}_w` are calculated\n",
    "    by the following formulas.\n",
    "    * :math:`\\\\hat{g}_y = p_h t_y + p_y`\n",
    "    * :math:`\\\\hat{g}_x = p_w t_x + p_x`\n",
    "    * :math:`\\\\hat{g}_h = p_h \\\\exp(t_h)`\n",
    "    * :math:`\\\\hat{g}_w = p_w \\\\exp(t_w)`\n",
    "    The decoding formulas are used in works such as R-CNN [#]_.\n",
    "    The output is same type as the type of the inputs.\n",
    "    .. [#] Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik. \\\n",
    "    Rich feature hierarchies for accurate object detection and semantic \\\n",
    "    segmentation. CVPR 2014.\n",
    "    Args:\n",
    "        src_bbox (array): A coordinates of bounding boxes.\n",
    "            Its shape is :math:`(R, 4)`. These coordinates are\n",
    "            :math:`p_{ymin}, p_{xmin}, p_{ymax}, p_{xmax}`.\n",
    "        loc (array): An array with offsets and scales.\n",
    "            The shapes of :obj:`src_bbox` and :obj:`loc` should be same.\n",
    "            This contains values :math:`t_y, t_x, t_h, t_w`.\n",
    "    Returns:\n",
    "        array:\n",
    "        Decoded bounding box coordinates. Its shape is :math:`(R, 4)`. \\\n",
    "        The second axis contains four values \\\n",
    "        :math:`\\\\hat{g}_{ymin}, \\\\hat{g}_{xmin},\n",
    "        \\\\hat{g}_{ymax}, \\\\hat{g}_{xmax}`.\n",
    "    \"\"\"\n",
    "\n",
    "    if src_bbox.shape[0] == 0:\n",
    "        return np.zeros((0, 4), dtype=loc.dtype)\n",
    "\n",
    "    src_bbox = src_bbox.astype(src_bbox.dtype, copy=False)\n",
    "\n",
    "    src_height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    src_width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    src_ctr_y = src_bbox[:, 0] + 0.5 * src_height\n",
    "    src_ctr_x = src_bbox[:, 1] + 0.5 * src_width\n",
    "\n",
    "    dy = loc[:, 0::4]\n",
    "    dx = loc[:, 1::4]\n",
    "    dh = loc[:, 2::4]\n",
    "    dw = loc[:, 3::4]\n",
    "\n",
    "    ctr_y = dy * src_height[:, np.newaxis] + src_ctr_y[:, np.newaxis]\n",
    "    ctr_x = dx * src_width[:, np.newaxis] + src_ctr_x[:, np.newaxis]\n",
    "    h = np.exp(dh) * src_height[:, np.newaxis]\n",
    "    w = np.exp(dw) * src_width[:, np.newaxis]\n",
    "\n",
    "    dst_bbox = np.zeros(loc.shape, dtype=loc.dtype)\n",
    "    dst_bbox[:, 0::4] = ctr_y - 0.5 * h\n",
    "    dst_bbox[:, 1::4] = ctr_x - 0.5 * w\n",
    "    dst_bbox[:, 2::4] = ctr_y + 0.5 * h\n",
    "    dst_bbox[:, 3::4] = ctr_x + 0.5 * w\n",
    "\n",
    "    return dst_bbox\n",
    "\n",
    "\n",
    "def bbox2loc(src_bbox, dst_bbox):\n",
    "    \"\"\"Encodes the source and the destination bounding boxes to \"loc\".\n",
    "    Given bounding boxes, this function computes offsets and scales\n",
    "    to match the source bounding boxes to the target bounding boxes.\n",
    "    Mathematcially, given a bounding box whose center is\n",
    "    :math:`(y, x) = p_y, p_x` and\n",
    "    size :math:`p_h, p_w` and the target bounding box whose center is\n",
    "    :math:`g_y, g_x` and size :math:`g_h, g_w`, the offsets and scales\n",
    "    :math:`t_y, t_x, t_h, t_w` can be computed by the following formulas.\n",
    "    * :math:`t_y = \\\\frac{(g_y - p_y)} {p_h}`\n",
    "    * :math:`t_x = \\\\frac{(g_x - p_x)} {p_w}`\n",
    "    * :math:`t_h = \\\\log(\\\\frac{g_h} {p_h})`\n",
    "    * :math:`t_w = \\\\log(\\\\frac{g_w} {p_w})`\n",
    "    The output is same type as the type of the inputs.\n",
    "    The encoding formulas are used in works such as R-CNN [#]_.\n",
    "    .. [#] Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik. \\\n",
    "    Rich feature hierarchies for accurate object detection and semantic \\\n",
    "    segmentation. CVPR 2014.\n",
    "    Args:\n",
    "        src_bbox (array): An image coordinate array whose shape is\n",
    "            :math:`(R, 4)`. :math:`R` is the number of bounding boxes.\n",
    "            These coordinates are\n",
    "            :math:`p_{ymin}, p_{xmin}, p_{ymax}, p_{xmax}`. -> 신경망을 통해서 나온 값(좌표)\n",
    "        dst_bbox (array): An image coordinate array whose shape is\n",
    "            :math:`(R, 4)`.\n",
    "            These coordinates are\n",
    "            :math:`g_{ymin}, g_{xmin}, g_{ymax}, g_{xmax}`. -> ground truth 좌표\n",
    "    Returns:\n",
    "        array:\n",
    "        Bounding box offsets and scales from :obj:`src_bbox` \\\n",
    "        to :obj:`dst_bbox`. \\\n",
    "        This has shape :math:`(R, 4)`.\n",
    "        The second axis contains four values :math:`t_y, t_x, t_h, t_w`. -> 두 입력 좌표를 통해 계산해서 regression으로 추론할 값 t\n",
    "    \"\"\"\n",
    "\n",
    "    height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    ctr_y = src_bbox[:, 0] + 0.5 * height\n",
    "    ctr_x = src_bbox[:, 1] + 0.5 * width\n",
    "\n",
    "    base_height = dst_bbox[:, 2] - dst_bbox[:, 0]\n",
    "    base_width = dst_bbox[:, 3] - dst_bbox[:, 1]\n",
    "    base_ctr_y = dst_bbox[:, 0] + 0.5 * base_height\n",
    "    base_ctr_x = dst_bbox[:, 1] + 0.5 * base_width\n",
    "\n",
    "    eps = np.finfo(height.dtype).eps\n",
    "    height = np.maximum(height, eps)\n",
    "    width = np.maximum(width, eps)\n",
    "\n",
    "    dy = (base_ctr_y - ctr_y) / height\n",
    "    dx = (base_ctr_x - ctr_x) / width\n",
    "    dh = np.log(base_height / height)\n",
    "    dw = np.log(base_width / width)\n",
    "\n",
    "    loc = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "    return loc\n",
    "\n",
    "def bbox_iou(bbox_a, bbox_b):\n",
    "    \"\"\"Calculate the Intersection of Unions (IoUs) between bounding boxes.\n",
    "    IoU is calculated as a ratio of area of the intersection\n",
    "    and area of the union.\n",
    "    This function accepts both :obj:`numpy.ndarray` and :obj:`cupy.ndarray` as\n",
    "    inputs. Please note that both :obj:`bbox_a` and :obj:`bbox_b` need to be\n",
    "    same type.\n",
    "    The output is same type as the type of the inputs.\n",
    "    Args:\n",
    "        bbox_a (array): An array whose shape is :math:`(N, 4)`.\n",
    "            :math:`N` is the number of bounding boxes.\n",
    "            The dtype should be :obj:`numpy.float32`.\n",
    "        bbox_b (array): An array similar to :obj:`bbox_a`,\n",
    "            whose shape is :math:`(K, 4)`.\n",
    "            The dtype should be :obj:`numpy.float32`.\n",
    "    Returns:\n",
    "        array:\n",
    "        An array whose shape is :math:`(N, K)`. \\\n",
    "        An element at index :math:`(n, k)` contains IoUs between \\\n",
    "        :math:`n` th bounding box in :obj:`bbox_a` and :math:`k` th bounding \\\n",
    "        box in :obj:`bbox_b`.\n",
    "    \"\"\"\n",
    "    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n",
    "        raise IndexError\n",
    "\n",
    "    #bbox_a 1개와 bbox_b k개를 비교해야하므로 None을 이용해서 차원을 늘려서 연산한다.\n",
    "    # top left\n",
    "    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n",
    "    # bottom right\n",
    "    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n",
    "\n",
    "    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n",
    "    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n",
    "    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n",
    "    return area_i / (area_a[:, None] + area_b - area_i)\n",
    "\n",
    "\n",
    "def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2],\n",
    "                         anchor_scales=[8, 16, 32]):\n",
    "    \"\"\"Generate anchor base windows by enumerating aspect ratio and scales.\n",
    "    Generate anchors that are scaled and modified to the given aspect ratios.\n",
    "    Area of a scaled anchor is preserved when modifying to the given aspect\n",
    "    ratio.\n",
    "    :obj:`R = len(ratios) * len(anchor_scales)` anchors are generated by this\n",
    "    function.\n",
    "    The :obj:`i * len(anchor_scales) + j` th anchor corresponds to an anchor\n",
    "    generated by :obj:`ratios[i]` and :obj:`anchor_scales[j]`.\n",
    "    For example, if the scale is :math:`8` and the ratio is :math:`0.25`,\n",
    "    the width and the height of the base window will be stretched by :math:`8`.\n",
    "    For modifying the anchor to the given aspect ratio,\n",
    "    the height is halved and the width is doubled.\n",
    "    Args:\n",
    "        base_size (number): The width and the height of the reference window.\n",
    "        ratios (list of floats): This is ratios of width to height of\n",
    "            the anchors.\n",
    "        anchor_scales (list of numbers): This is areas of anchors.\n",
    "            Those areas will be the product of the square of an element in\n",
    "            :obj:`anchor_scales` and the original area of the reference\n",
    "            window.\n",
    "    Returns:\n",
    "        ~numpy.ndarray:\n",
    "        An array of shape :math:`(R, 4)`.\n",
    "        Each element is a set of coordinates of a bounding box.\n",
    "        The second axis corresponds to\n",
    "        :math:`(y_{min}, x_{min}, y_{max}, x_{max})` of a bounding box.\n",
    "    \"\"\"\n",
    "    py = base_size / 2.\n",
    "    px = base_size / 2.\n",
    "\n",
    "    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4),\n",
    "                           dtype=np.float32)\n",
    "    for i in six.moves.range(len(ratios)):\n",
    "        for j in six.moves.range(len(anchor_scales)):\n",
    "            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])\n",
    "\n",
    "            index = i * len(anchor_scales) + j\n",
    "            anchor_base[index, 0] = py - h / 2.\n",
    "            anchor_base[index, 1] = px - w / 2.\n",
    "            anchor_base[index, 2] = py + h / 2.\n",
    "            anchor_base[index, 3] = px + w / 2.\n",
    "    return anchor_base\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNN(nn.Module):\n",
    "    \"\"\"Base class for Faster R-CNN.\n",
    "    This is a base class for Faster R-CNN links supporting object detection\n",
    "    API [#]_. The following three stages constitute Faster R-CNN.\n",
    "    1. **Feature extraction**: Images are taken and their \\\n",
    "        feature maps are calculated.\n",
    "    2. **Region Proposal Networks**: Given the feature maps calculated in \\\n",
    "        the previous stage, produce set of RoIs around objects.\n",
    "    3. **Localization and Classification Heads**: Using feature maps that \\\n",
    "        belong to the proposed RoIs, classify the categories of the objects \\\n",
    "        in the RoIs and improve localizations.\n",
    "    Each stage is carried out by one of the callable\n",
    "    :class:`torch.nn.Module` objects :obj:`feature`, :obj:`rpn` and :obj:`head`.\n",
    "    There are two functions :meth:`predict` and :meth:`__call__` to conduct\n",
    "    object detection.\n",
    "    :meth:`predict` takes images and returns bounding boxes that are converted\n",
    "    to image coordinates. This will be useful for a scenario when\n",
    "    Faster R-CNN is treated as a black box function, for instance.\n",
    "    :meth:`__call__` is provided for a scnerario when intermediate outputs\n",
    "    are needed, for instance, for training and debugging.\n",
    "    Links that support obejct detection API have method :meth:`predict` with\n",
    "    the same interface. Please refer to :meth:`predict` for\n",
    "    further details.\n",
    "    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n",
    "    Faster R-CNN: Towards Real-Time Object Detection with \\\n",
    "    Region Proposal Networks. NIPS 2015.\n",
    "    Args:\n",
    "        extractor (nn.Module): A module that takes a BCHW image\n",
    "            array and returns feature maps.\n",
    "        rpn (nn.Module): A module that has the same interface as\n",
    "            :class:`model.region_proposal_network.RegionProposalNetwork`.\n",
    "            Please refer to the documentation found there.\n",
    "        head (nn.Module): A module that takes\n",
    "            a BCHW variable, RoIs and batch indices for RoIs. This returns class\n",
    "            dependent localization paramters and class scores.\n",
    "        loc_normalize_mean (tuple of four floats): Mean values of\n",
    "            localization estimates.\n",
    "        loc_normalize_std (tupler of four floats): Standard deviation\n",
    "            of localization estimates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, extractor, rpn, head,\n",
    "                loc_normalize_mean = (0., 0., 0., 0.),\n",
    "                loc_normalize_std = (0.1, 0.1, 0.2, 0.2)\n",
    "    ):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        self.extractor = extractor\n",
    "        self.rpn = rpn\n",
    "        self.head = head\n",
    "\n",
    "        # mean and std\n",
    "        self.loc_normalize_mean = loc_normalize_mean\n",
    "        self.loc_normalize_std = loc_normalize_std\n",
    "#         self.use_preset('evaluate')\n",
    "\n",
    "    @property\n",
    "    def n_class(self):\n",
    "        # Total number of classes including the background.\n",
    "        return self.head.n_class\n",
    "\n",
    "    def forward(self, x, scale=1.):\n",
    "        \"\"\"Forward Faster R-CNN.\n",
    "        Scaling paramter :obj:`scale` is used by RPN to determine the\n",
    "        threshold to select small objects, which are going to be\n",
    "        rejected irrespective of their confidence scores.\n",
    "        Here are notations used.\n",
    "        * :math:`N` is the number of batch size\n",
    "        * :math:`R'` is the total number of RoIs produced across batches. \\\n",
    "            Given :math:`R_i` proposed RoIs from the :math:`i` th image, \\\n",
    "            :math:`R' = \\\\sum _{i=1} ^ N R_i`.\n",
    "        * :math:`L` is the number of classes excluding the background.\n",
    "        Classes are ordered by the background, the first class, ..., and\n",
    "        the :math:`L` th class.\n",
    "        Args:\n",
    "            x (autograd.Variable): 4D image variable.\n",
    "            scale (float): Amount of scaling applied to the raw image\n",
    "                during preprocessing.\n",
    "        Returns:\n",
    "            Variable, Variable, array, array:\n",
    "            Returns tuple of four values listed below.\n",
    "            * **roi_cls_locs**: Offsets and scalings for the proposed RoIs. \\\n",
    "                Its shape is :math:`(R', (L + 1) \\\\times 4)`.\n",
    "            * **roi_scores**: Class predictions for the proposed RoIs. \\\n",
    "                Its shape is :math:`(R', L + 1)`.\n",
    "            * **rois**: RoIs proposed by RPN. Its shape is \\\n",
    "                :math:`(R', 4)`.\n",
    "            * **roi_indices**: Batch indices of RoIs. Its shape is \\\n",
    "                :math:`(R',)`.\n",
    "        \"\"\"\n",
    "        img_size = x.shape[2:]\n",
    "\n",
    "        h = self.extractor(x)\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, anchor = \\\n",
    "            self.rpn(h, img_size, scale)\n",
    "        roi_cls_locs, roi_scores = self.head(\n",
    "            h, rois, roi_indices)\n",
    "        return roi_cls_locs, roi_scores, rois, roi_indices #RoI loss(fast R-CNN loss)를 구할때 사용되는 값들 -> 즉 해당 class는 fast R-CNN의 출력을 구하기 위한 class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.ops import nms\n",
    "# from model.utils.bbox_tools import bbox2loc, bbox_iou, loc2bbox\n",
    "\n",
    "\n",
    "class ProposalTargetCreator:\n",
    "    \"\"\"roi(nms를 거친 roi)에 해당되는 ground truth와 iou를 비교하여 먼저 positive/negative를 sampling한다(논문 기준 128개) -> sample_roi\n",
    "\n",
    "    해당 객체 클래스에 맞게 labeling하고(배경:0, 클래스1:1, ..., 클래스n:n) sample_roi의 인덱스와 맞는 label들만 사용한다.\n",
    "    sample roi와 gt_bbox를 이용해 bbox regression에서 regression해야할 ground truth loc값 t_x, t_y, t_w, t_h 을 구한다.\n",
    "    따라서 return으로 sample_roi, ground truth loc와 label이다.\n",
    "    이를 이용해 fast R-CNN loss(RoI loss)를 계산할 때, sample_roi는 네트워크 입력으로, loc와 label은 gt로 사용한다.\n",
    "    Assign ground truth bounding boxes to given RoIs.\n",
    "    The :meth:`__call__` of this class generates training targets\n",
    "    for each object proposal.\n",
    "    This is used to train Faster RCNN [#]_.\n",
    "    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n",
    "    Faster R-CNN: Towards Real-Time Object Detection with \\\n",
    "    Region Proposal Networks. NIPS 2015.\n",
    "    Args:\n",
    "        n_sample (int): The number of sampled regions.\n",
    "        pos_ratio (float): Fraction of regions that is labeled as a\n",
    "            foreground.\n",
    "        pos_iou_thresh (float): IoU threshold for a RoI to be considered as a\n",
    "            foreground.\n",
    "        neg_iou_thresh_hi (float): RoI is considered to be the background\n",
    "            if IoU is in\n",
    "            [:obj:`neg_iou_thresh_hi`, :obj:`neg_iou_thresh_hi`).\n",
    "        neg_iou_thresh_lo (float): See above.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_sample=128,\n",
    "                 pos_ratio=0.25, pos_iou_thresh=0.5,\n",
    "                 neg_iou_thresh_hi=0.5, neg_iou_thresh_lo=0.0\n",
    "                 ):\n",
    "        self.n_sample = n_sample\n",
    "        self.pos_ratio = pos_ratio\n",
    "        self.pos_iou_thresh = pos_iou_thresh\n",
    "        self.neg_iou_thresh_hi = neg_iou_thresh_hi\n",
    "        self.neg_iou_thresh_lo = neg_iou_thresh_lo  # NOTE:default 0.1 in py-faster-rcnn\n",
    "\n",
    "    def __call__(self, roi, bbox, label,\n",
    "                 loc_normalize_mean=(0., 0., 0., 0.),\n",
    "                 loc_normalize_std=(0.1, 0.1, 0.2, 0.2)):\n",
    "        \"\"\"Assigns ground truth to sampled proposals.\n",
    "        This function samples total of :obj:`self.n_sample` RoIs\n",
    "        from the combination of :obj:`roi` and :obj:`bbox`.\n",
    "        The RoIs are assigned with the ground truth class labels as well as\n",
    "        bounding box offsets and scales to match the ground truth bounding\n",
    "        boxes. As many as :obj:`pos_ratio * self.n_sample` RoIs are\n",
    "        sampled as foregrounds.\n",
    "        Offsets and scales of bounding boxes are calculated using\n",
    "        :func:`model.utils.bbox_tools.bbox2loc`.\n",
    "        Also, types of input arrays and output arrays are same.\n",
    "        Here are notations.\n",
    "        * :math:`S` is the total number of sampled RoIs, which equals \\\n",
    "            :obj:`self.n_sample`.\n",
    "        * :math:`L` is number of object classes possibly including the \\\n",
    "            background.\n",
    "        Args:\n",
    "            roi (array): Region of Interests (RoIs) from which we sample.\n",
    "                Its shape is :math:`(R, 4)`\n",
    "            bbox (array): The coordinates of ground truth bounding boxes.\n",
    "                Its shape is :math:`(R', 4)`.\n",
    "            label (array): Ground truth bounding box labels. Its shape\n",
    "                is :math:`(R',)`. Its range is :math:`[0, L - 1]`, where\n",
    "                :math:`L` is the number of foreground classes.\n",
    "            loc_normalize_mean (tuple of four floats): Mean values to normalize\n",
    "                coordinates of bouding boxes.\n",
    "            loc_normalize_std (tupler of four floats): Standard deviation of\n",
    "                the coordinates of bounding boxes.\n",
    "        Returns:\n",
    "            (array, array, array):\n",
    "            * **sample_roi**: Regions of interests that are sampled. \\\n",
    "                Its shape is :math:`(S, 4)`.\n",
    "            * **gt_roi_loc**: Offsets and scales to match \\\n",
    "                the sampled RoIs to the ground truth bounding boxes. \\\n",
    "                Its shape is :math:`(S, 4)`.\n",
    "            * **gt_roi_label**: Labels assigned to sampled RoIs. Its shape is \\\n",
    "                :math:`(S,)`. Its range is :math:`[0, L]`. The label with \\\n",
    "                value 0 is the background.\n",
    "        \"\"\"\n",
    "        n_bbox, _ = bbox.shape\n",
    "\n",
    "        roi = np.concatenate((roi, bbox), axis=0)\n",
    "\n",
    "        pos_roi_per_image = np.round(self.n_sample * self.pos_ratio)\n",
    "        iou = bbox_iou(roi, bbox)\n",
    "        gt_assignment = iou.argmax(axis=1)\n",
    "        max_iou = iou.max(axis=1)\n",
    "        # Offset range of classes from [0, n_fg_class - 1] to [1, n_fg_class].\n",
    "        # The label with value 0 is the background.\n",
    "        gt_roi_label = label[gt_assignment] + 1\n",
    "\n",
    "        # Select foreground RoIs as those with >= pos_iou_thresh IoU.\n",
    "        pos_index = np.where(max_iou >= self.pos_iou_thresh)[0]\n",
    "        pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
    "        if pos_index.size > 0:\n",
    "            pos_index = np.random.choice(\n",
    "                pos_index, size=pos_roi_per_this_image, replace=False)\n",
    "\n",
    "        # Select background RoIs as those within\n",
    "        # [neg_iou_thresh_lo, neg_iou_thresh_hi).\n",
    "        neg_index = np.where((max_iou < self.neg_iou_thresh_hi) &\n",
    "                             (max_iou >= self.neg_iou_thresh_lo))[0]\n",
    "        neg_roi_per_this_image = self.n_sample - pos_roi_per_this_image\n",
    "        neg_roi_per_this_image = int(min(neg_roi_per_this_image,\n",
    "                                         neg_index.size))\n",
    "        if neg_index.size > 0:\n",
    "            neg_index = np.random.choice(\n",
    "                neg_index, size=neg_roi_per_this_image, replace=False)\n",
    "\n",
    "        # The indices that we're selecting (both positive and negative).\n",
    "        keep_index = np.append(pos_index, neg_index)\n",
    "        gt_roi_label = gt_roi_label[keep_index]\n",
    "        gt_roi_label[pos_roi_per_this_image:] = 0  # negative labels --> 0\n",
    "        sample_roi = roi[keep_index]\n",
    "\n",
    "        # Compute offsets and scales to match sampled RoIs to the GTs.\n",
    "        gt_roi_loc = bbox2loc(sample_roi, bbox[gt_assignment[keep_index]])\n",
    "        gt_roi_loc = ((gt_roi_loc - np.array(loc_normalize_mean, np.float32)\n",
    "                       ) / np.array(loc_normalize_std, np.float32))\n",
    "\n",
    "        return sample_roi, gt_roi_loc, gt_roi_label\n",
    "\n",
    "\n",
    "class AnchorTargetCreator(object):\n",
    "    \"\"\"Anchor에 해당되는 ground truth와 iou를 비교하여 positive, negative, ignore sample을 labeling하고\n",
    "    bbox regression에서 regression해야할 ground truth loc값 t_x, t_y, t_w, t_h 을 구한다.\n",
    "    따라서 return으로 ground truth loc와 label이다.\n",
    "    이를 이용해 RPN loss를 계산할 때, gt로 사용한다.\n",
    "    Assign the ground truth bounding boxes to anchors.\n",
    "    Assigns the ground truth bounding boxes to anchors for training Region\n",
    "    Proposal Networks introduced in Faster R-CNN [#]_.\n",
    "    Offsets and scales to match anchors to the ground truth are\n",
    "    calculated using the encoding scheme of\n",
    "    :func:`model.utils.bbox_tools.bbox2loc`.\n",
    "    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n",
    "    Faster R-CNN: Towards Real-Time Object Detection with \\\n",
    "    Region Proposal Networks. NIPS 2015.\n",
    "    Args:\n",
    "        n_sample (int): The number of regions to produce.\n",
    "        pos_iou_thresh (float): Anchors with IoU above this\n",
    "            threshold will be assigned as positive.\n",
    "        neg_iou_thresh (float): Anchors with IoU below this\n",
    "            threshold will be assigned as negative.\n",
    "        pos_ratio (float): Ratio of positive regions in the\n",
    "            sampled regions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_sample=256,\n",
    "                 pos_iou_thresh=0.7, neg_iou_thresh=0.3,\n",
    "                 pos_ratio=0.5):\n",
    "        self.n_sample = n_sample\n",
    "        self.pos_iou_thresh = pos_iou_thresh\n",
    "        self.neg_iou_thresh = neg_iou_thresh\n",
    "        self.pos_ratio = pos_ratio\n",
    "\n",
    "    def __call__(self, bbox, anchor, img_size):\n",
    "        \"\"\"Assign ground truth supervision to sampled subset of anchors.\n",
    "        Types of input arrays and output arrays are same.\n",
    "        Here are notations.\n",
    "        * :math:`S` is the number of anchors.\n",
    "        * :math:`R` is the number of bounding boxes.\n",
    "        Args:\n",
    "            bbox (array): Coordinates of bounding boxes. Its shape is\n",
    "                :math:`(R, 4)`.\n",
    "            anchor (array): Coordinates of anchors. Its shape is\n",
    "                :math:`(S, 4)`.\n",
    "            img_size (tuple of ints): A tuple :obj:`H, W`, which\n",
    "                is a tuple of height and width of an image.\n",
    "        Returns:\n",
    "            (array, array):\n",
    "            #NOTE: it's scale not only  offset\n",
    "            * **loc**: Offsets and scales to match the anchors to \\\n",
    "                the ground truth bounding boxes. Its shape is :math:`(S, 4)`.\n",
    "            * **label**: Labels of anchors with values \\\n",
    "                :obj:`(1=positive, 0=negative, -1=ignore)`. Its shape \\\n",
    "                is :math:`(S,)`.\n",
    "        \"\"\"\n",
    "\n",
    "        img_H, img_W = img_size\n",
    "\n",
    "        n_anchor = len(anchor)\n",
    "        inside_index = _get_inside_index(anchor, img_H, img_W)\n",
    "        anchor = anchor[inside_index]\n",
    "        argmax_ious, label = self._create_label(\n",
    "            inside_index, anchor, bbox)\n",
    "\n",
    "        # compute bounding box regression targets\n",
    "        loc = bbox2loc(anchor, bbox[argmax_ious])\n",
    "\n",
    "        # map up to original set of anchors\n",
    "        label = _unmap(label, n_anchor, inside_index, fill=-1)\n",
    "        loc = _unmap(loc, n_anchor, inside_index, fill=0)\n",
    "\n",
    "        return loc, label\n",
    "\n",
    "    def _create_label(self, inside_index, anchor, bbox):\n",
    "        # label: 1 is positive, 0 is negative, -1 is dont care\n",
    "        label = np.empty((len(inside_index),), dtype=np.int32)\n",
    "        label.fill(-1)\n",
    "\n",
    "        argmax_ious, max_ious, gt_argmax_ious = \\\n",
    "            self._calc_ious(anchor, bbox, inside_index)\n",
    "\n",
    "        # assign negative labels first so that positive labels can clobber them\n",
    "        label[max_ious < self.neg_iou_thresh] = 0\n",
    "\n",
    "        # positive label: for each gt, anchor with highest iou\n",
    "        label[gt_argmax_ious] = 1\n",
    "\n",
    "        # positive label: above threshold IOU\n",
    "        label[max_ious >= self.pos_iou_thresh] = 1\n",
    "\n",
    "        # subsample positive labels if we have too many\n",
    "        n_pos = int(self.pos_ratio * self.n_sample)\n",
    "        pos_index = np.where(label == 1)[0]\n",
    "        if len(pos_index) > n_pos:\n",
    "            disable_index = np.random.choice(\n",
    "                pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
    "            label[disable_index] = -1\n",
    "\n",
    "        # subsample negative labels if we have too many\n",
    "        n_neg = self.n_sample - np.sum(label == 1)\n",
    "        neg_index = np.where(label == 0)[0]\n",
    "        if len(neg_index) > n_neg:\n",
    "            disable_index = np.random.choice(\n",
    "                neg_index, size=(len(neg_index) - n_neg), replace=False)\n",
    "            label[disable_index] = -1\n",
    "\n",
    "        return argmax_ious, label\n",
    "\n",
    "    def _calc_ious(self, anchor, bbox, inside_index):\n",
    "        # ious between the anchors and the gt boxes\n",
    "        ious = bbox_iou(anchor, bbox)\n",
    "        argmax_ious = ious.argmax(axis=1)\n",
    "        max_ious = ious[np.arange(len(inside_index)), argmax_ious]\n",
    "        gt_argmax_ious = ious.argmax(axis=0)\n",
    "        gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
    "        gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
    "\n",
    "        return argmax_ious, max_ious, gt_argmax_ious\n",
    "\n",
    "\n",
    "def _unmap(data, count, index, fill=0):\n",
    "    # Unmap a subset of item (data) back to the original set of items (of\n",
    "    # size count)\n",
    "\n",
    "    if len(data.shape) == 1:\n",
    "        ret = np.empty((count,), dtype=data.dtype)\n",
    "        ret.fill(fill)\n",
    "        ret[index] = data\n",
    "    else:\n",
    "        ret = np.empty((count,) + data.shape[1:], dtype=data.dtype)\n",
    "        ret.fill(fill)\n",
    "        ret[index, :] = data\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _get_inside_index(anchor, H, W):\n",
    "    # Calc indicies of anchors which are located completely inside of the image\n",
    "    # whose size is speficied.\n",
    "    index_inside = np.where(\n",
    "        (anchor[:, 0] >= 0) &\n",
    "        (anchor[:, 1] >= 0) &\n",
    "        (anchor[:, 2] <= H) &\n",
    "        (anchor[:, 3] <= W)\n",
    "    )[0]\n",
    "    return index_inside\n",
    "\n",
    "\n",
    "class ProposalCreator:\n",
    "    # unNOTE: I'll make it undifferential\n",
    "    # unTODO: make sure it's ok\n",
    "    # It's ok\n",
    "    \"\"\"feature map -> (conv3x3, regression conv) -> loc가 나오게 된다.\n",
    "    해당 loc들을 anchor와 함께 함수를 통과시켜 roi로(bbox 형태의 좌표)로 바꾼다. 이렇게 나온 roi들을\n",
    "    미리 정한 크기 조건에 맞는 roi들만을 남긴다. 다시 이 중에서 non-maximum-suppression을\n",
    "    이용해 미리 정한 개수(train:2000, test:300)만 남겨 roi를 반환한다.\n",
    "    Proposal regions are generated by calling this object.\n",
    "    The :meth:`__call__` of this object outputs object detection proposals by\n",
    "    applying estimated bounding box offsets\n",
    "    to a set of anchors.\n",
    "    This class takes parameters to control number of bounding boxes to\n",
    "    pass to NMS and keep after NMS.\n",
    "    If the paramters are negative, it uses all the bounding boxes supplied\n",
    "    or keep all the bounding boxes returned by NMS.\n",
    "    This class is used for Region Proposal Networks introduced in\n",
    "    Faster R-CNN [#]_.\n",
    "    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n",
    "    Faster R-CNN: Towards Real-Time Object Detection with \\\n",
    "    Region Proposal Networks. NIPS 2015.\n",
    "    Args:\n",
    "        nms_thresh (float): Threshold value used when calling NMS.\n",
    "        n_train_pre_nms (int): Number of top scored bounding boxes\n",
    "            to keep before passing to NMS in train mode.\n",
    "        n_train_post_nms (int): Number of top scored bounding boxes\n",
    "            to keep after passing to NMS in train mode.\n",
    "        n_test_pre_nms (int): Number of top scored bounding boxes\n",
    "            to keep before passing to NMS in test mode.\n",
    "        n_test_post_nms (int): Number of top scored bounding boxes\n",
    "            to keep after passing to NMS in test mode.\n",
    "        force_cpu_nms (bool): If this is :obj:`True`,\n",
    "            always use NMS in CPU mode. If :obj:`False`,\n",
    "            the NMS mode is selected based on the type of inputs.\n",
    "        min_size (int): A paramter to determine the threshold on\n",
    "            discarding bounding boxes based on their sizes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 parent_model,\n",
    "                 nms_thresh=0.7,\n",
    "                 n_train_pre_nms=12000,\n",
    "                 n_train_post_nms=2000,\n",
    "                 n_test_pre_nms=6000,\n",
    "                 n_test_post_nms=300,\n",
    "                 min_size=16\n",
    "                 ):\n",
    "        self.parent_model = parent_model\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.n_train_pre_nms = n_train_pre_nms\n",
    "        self.n_train_post_nms = n_train_post_nms\n",
    "        self.n_test_pre_nms = n_test_pre_nms\n",
    "        self.n_test_post_nms = n_test_post_nms\n",
    "        self.min_size = min_size\n",
    "\n",
    "    def __call__(self, loc, score,\n",
    "                 anchor, img_size, scale=1.):\n",
    "        \"\"\"input should  be ndarray\n",
    "        Propose RoIs.\n",
    "        Inputs :obj:`loc, score, anchor` refer to the same anchor when indexed\n",
    "        by the same index.\n",
    "        On notations, :math:`R` is the total number of anchors. This is equal\n",
    "        to product of the height and the width of an image and the number of\n",
    "        anchor bases per pixel.\n",
    "        Type of the output is same as the inputs.\n",
    "        Args:\n",
    "            loc (array): Predicted offsets and scaling to anchors.\n",
    "                Its shape is :math:`(R, 4)`.\n",
    "            score (array): Predicted foreground probability for anchors.\n",
    "                Its shape is :math:`(R,)`.\n",
    "            anchor (array): Coordinates of anchors. Its shape is\n",
    "                :math:`(R, 4)`.\n",
    "            img_size (tuple of ints): A tuple :obj:`height, width`,\n",
    "                which contains image size after scaling.\n",
    "            scale (float): The scaling factor used to scale an image after\n",
    "                reading it from a file.\n",
    "        Returns:\n",
    "            array:\n",
    "            An array of coordinates of proposal boxes.\n",
    "            Its shape is :math:`(S, 4)`. :math:`S` is less than\n",
    "            :obj:`self.n_test_post_nms` in test time and less than\n",
    "            :obj:`self.n_train_post_nms` in train time. :math:`S` depends on\n",
    "            the size of the predicted bounding boxes and the number of\n",
    "            bounding boxes discarded by NMS.\n",
    "        \"\"\"\n",
    "        # NOTE: when test, remember\n",
    "        # faster_rcnn.eval()\n",
    "        # to set self.traing = False\n",
    "        if self.parent_model.training:\n",
    "            n_pre_nms = self.n_train_pre_nms\n",
    "            n_post_nms = self.n_train_post_nms\n",
    "        else:\n",
    "            n_pre_nms = self.n_test_pre_nms\n",
    "            n_post_nms = self.n_test_post_nms\n",
    "\n",
    "        # Convert anchors into proposal via bbox transformations.\n",
    "        roi = loc2bbox(anchor, loc)\n",
    "\n",
    "        # Clip predicted boxes to image.\n",
    "        roi[:, slice(0, 4, 2)] = np.clip(\n",
    "            roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
    "        roi[:, slice(1, 4, 2)] = np.clip(\n",
    "            roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
    "\n",
    "        # Remove predicted boxes with either height or width < threshold.\n",
    "        min_size = self.min_size * scale\n",
    "        hs = roi[:, 2] - roi[:, 0]\n",
    "        ws = roi[:, 3] - roi[:, 1]\n",
    "        keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
    "        roi = roi[keep, :]\n",
    "        score = score[keep]\n",
    "\n",
    "        # Sort all (proposal, score) pairs by score from highest to lowest.\n",
    "        # Take top pre_nms_topN (e.g. 6000).\n",
    "        order = score.ravel().argsort()[::-1]\n",
    "        if n_pre_nms > 0:\n",
    "            order = order[:n_pre_nms]\n",
    "        roi = roi[order, :]\n",
    "        score = score[order]\n",
    "\n",
    "        # Apply nms (e.g. threshold = 0.7).\n",
    "        # Take after_nms_topN (e.g. 300).\n",
    "\n",
    "        # unNOTE: somthing is wrong here!\n",
    "        # TODO: remove cuda.to_gpu\n",
    "        keep = nms(\n",
    "            torch.from_numpy(roi).cuda(),\n",
    "            torch.from_numpy(score).cuda(),\n",
    "            self.nms_thresh)\n",
    "        if n_post_nms > 0:\n",
    "            keep = keep[:n_post_nms]\n",
    "        roi = roi[keep.cpu().numpy()]\n",
    "        return roi #nms를 거친 roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from torch.nn import functional as F\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# from model.utils.bbox_tools import generate_anchor_base\n",
    "# from model.utils.creator_tool import ProposalCreator\n",
    "\n",
    "class RegionProposalNetwork(nn.Module):\n",
    "    \"\"\"Region Proposal Network introduced in Faster R-CNN.\n",
    "    This is Region Proposal Network introduced in Faster R-CNN [#]_.\n",
    "    This takes features extracted from images and propose\n",
    "    class agnostic bounding boxes around \"objects\".\n",
    "    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n",
    "    Faster R-CNN: Towards Real-Time Object Detection with \\\n",
    "    Region Proposal Networks. NIPS 2015.\n",
    "    Args:\n",
    "        in_channels (int): The channel size of input.\n",
    "        mid_channels (int): The channel size of the intermediate tensor.\n",
    "        ratios (list of floats): This is ratios of width to height of\n",
    "            the anchors.\n",
    "        anchor_scales (list of numbers): This is areas of anchors.\n",
    "            Those areas will be the product of the square of an element in\n",
    "            :obj:`anchor_scales` and the original area of the reference\n",
    "            window.\n",
    "        feat_stride (int): Stride size after extracting features from an\n",
    "            image.\n",
    "        initialW (callable): Initial weight value. If :obj:`None` then this\n",
    "            function uses Gaussian distribution scaled by 0.1 to\n",
    "            initialize weight.\n",
    "            May also be a callable that takes an array and edits its values.\n",
    "        proposal_creator_params (dict): Key valued paramters for\n",
    "            :class:`model.utils.creator_tools.ProposalCreator`.\n",
    "    .. seealso::\n",
    "        :class:`~model.utils.creator_tools.ProposalCreator`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, in_channels=512, mid_channels=512, ratios=[0.5, 1, 2],\n",
    "            anchor_scales=[8, 16, 32], feat_stride=16,\n",
    "            proposal_creator_params=dict(),\n",
    "    ):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        self.anchor_base = generate_anchor_base(\n",
    "            anchor_scales=anchor_scales, ratios=ratios) #한 anchor에서 9개의 anchor box 생성\n",
    "        self.feat_stride = feat_stride\n",
    "        self.proposal_layer = ProposalCreator(self, **proposal_creator_params) #여기서 self는 RPN이 되고 proposalcreator의 인자로 들어가서 해당 네트워크가 training인지 testing인지 알려준다.\n",
    "        n_anchor = self.anchor_base.shape[0]\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
    "        self.score = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0)\n",
    "        self.loc = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0)\n",
    "        normal_init(self.conv1, 0, 0.01)\n",
    "        normal_init(self.score, 0, 0.01)\n",
    "        normal_init(self.loc, 0, 0.01)\n",
    "\n",
    "    def forward(self, x, img_size, scale=1.):\n",
    "        \"\"\"Forward Region Proposal Network.\n",
    "        Here are notations.\n",
    "        * :math:`N` is batch size.\n",
    "        * :math:`C` channel size of the input.\n",
    "        * :math:`H` and :math:`W` are height and witdh of the input feature.\n",
    "        * :math:`A` is number of anchors assigned to each pixel.\n",
    "        Args:\n",
    "            x (~torch.autograd.Variable): The Features extracted from images.\n",
    "                Its shape is :math:`(N, C, H, W)`.\n",
    "            img_size (tuple of ints): A tuple :obj:`height, width`,\n",
    "                which contains image size after scaling.\n",
    "            scale (float): The amount of scaling done to the input images after\n",
    "                reading them from files.\n",
    "        Returns:\n",
    "            (~torch.autograd.Variable, ~torch.autograd.Variable, array, array, array):\n",
    "            This is a tuple of five following values.\n",
    "            * **rpn_locs**: Predicted bounding box offsets and scales for \\\n",
    "                anchors. Its shape is :math:`(N, H W A, 4)`.\n",
    "            * **rpn_scores**:  Predicted foreground scores for \\\n",
    "                anchors. Its shape is :math:`(N, H W A, 2)`.\n",
    "            * **rois**: A bounding box array containing coordinates of \\\n",
    "                proposal boxes.  This is a concatenation of bounding box \\\n",
    "                arrays from multiple images in the batch. \\\n",
    "                Its shape is :math:`(R', 4)`. Given :math:`R_i` predicted \\\n",
    "                bounding boxes from the :math:`i` th image, \\\n",
    "                :math:`R' = \\\\sum _{i=1} ^ N R_i`.\n",
    "            * **roi_indices**: An array containing indices of images to \\\n",
    "                which RoIs correspond to. Its shape is :math:`(R',)`.\n",
    "            * **anchor**: Coordinates of enumerated shifted anchors. \\\n",
    "                Its shape is :math:`(H W A, 4)`.\n",
    "        \"\"\"\n",
    "        n, _, hh, ww = x.shape\n",
    "        anchor = _enumerate_shifted_anchor(\n",
    "            np.array(self.anchor_base),\n",
    "            self.feat_stride, hh, ww) #이를 50x50의 feature map에 적용 -> 50*50*9 = 22500개의 anchor box\n",
    "\n",
    "        n_anchor = anchor.shape[0] // (hh * ww)\n",
    "        h = F.relu(self.conv1(x))\n",
    "\n",
    "        rpn_locs = self.loc(h)\n",
    "        # UNNOTE: check whether need contiguous\n",
    "        # A: Yes\n",
    "        rpn_locs = rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4)\n",
    "        rpn_scores = self.score(h)\n",
    "        rpn_scores = rpn_scores.permute(0, 2, 3, 1).contiguous()\n",
    "        rpn_softmax_scores = F.softmax(rpn_scores.view(n, hh, ww, n_anchor, 2), dim=4)\n",
    "        rpn_fg_scores = rpn_softmax_scores[:, :, :, :, 1].contiguous()\n",
    "        rpn_fg_scores = rpn_fg_scores.view(n, -1)\n",
    "        rpn_scores = rpn_scores.view(n, -1, 2)\n",
    "\n",
    "        rois = list()\n",
    "        roi_indices = list()\n",
    "        for i in range(n):\n",
    "            roi = self.proposal_layer(\n",
    "                rpn_locs[i].cpu().data.numpy(),\n",
    "                rpn_fg_scores[i].cpu().data.numpy(),\n",
    "                anchor, img_size,\n",
    "                scale=scale) #rpn_loc과 anchor를 loc2bbox 함수를 이용해 바꿔 RoI의 좌표를 뽑아낸다. 이때 크기를 고려하고, nms를 적용하여 개수를 줄인다.\n",
    "            batch_index = i * np.ones((len(roi),), dtype=np.int32)\n",
    "            rois.append(roi)\n",
    "            roi_indices.append(batch_index)\n",
    "\n",
    "        rois = np.concatenate(rois, axis=0)\n",
    "        roi_indices = np.concatenate(roi_indices, axis=0)\n",
    "        return rpn_locs, rpn_scores, rois, roi_indices, anchor\n",
    "\n",
    "\n",
    "def _enumerate_shifted_anchor(anchor_base, feat_stride, height, width):\n",
    "    # Enumerate all shifted anchors:\n",
    "    #\n",
    "    # anchor_base는 하나의 anchor에 9개 종류의 anchor를 갖고, 이를 enumerate시켜 전체 이미지에 각각의 anchor를 갖게 한다\n",
    "    #즉 50x50의 feature map에서 각각의 pixel은 하나의 anchor를 나타내므로 50x50x9개의 anchor box의 좌표를 구해준다.\n",
    "    # add A anchors (1, A, 4) to\n",
    "    # cell K shifts (K, 1, 4) to get\n",
    "    # shift anchors (K, A, 4)\n",
    "    # reshape to (K*A, 4) shifted anchors\n",
    "    # return (K*A, 4)\n",
    "\n",
    "    shift_y = np.arange(0, height * feat_stride, feat_stride)\n",
    "    shift_x = np.arange(0, width * feat_stride, feat_stride)\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "    shift = np.stack((shift_y.ravel(), shift_x.ravel(),\n",
    "                      shift_y.ravel(), shift_x.ravel()), axis=1)\n",
    "\n",
    "    A = anchor_base.shape[0]\n",
    "    K = shift.shape[0]\n",
    "    anchor = anchor_base.reshape((1, A, 4)) + \\\n",
    "             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n",
    "    return anchor\n",
    "\n",
    "\n",
    "def normal_init(m, mean, stddev, truncated=False):\n",
    "    \"\"\"\n",
    "    weight initalizer: truncated normal and random normal.\n",
    "    \"\"\"\n",
    "    # x is a parameter\n",
    "    if truncated:\n",
    "        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)  # not a perfect approximation\n",
    "    else:\n",
    "        m.weight.data.normal_(mean, stddev)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def decom_vgg16():\n",
    "    # the 30th layer of features is relu of conv5_3\n",
    "    if opt.caffe_pretrain:\n",
    "        model = vgg16(pretrained=False)\n",
    "        if not opt.load_path:\n",
    "            model.load_state_dict(t.load(opt.caffe_pretrain_path))\n",
    "    else:\n",
    "        model = vgg16(not opt.load_path)\n",
    "\n",
    "    features = list(model.features)[:30]\n",
    "    classifier = model.classifier\n",
    "\n",
    "    classifier = list(classifier)\n",
    "    del classifier[6]\n",
    "    if not opt.use_drop:\n",
    "        del classifier[5]\n",
    "        del classifier[2]\n",
    "    classifier = nn.Sequential(*classifier)\n",
    "\n",
    "    # freeze top4 conv\n",
    "    for layer in features[:10]:\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    return nn.Sequential(*features), classifier\n",
    "\n",
    "class FasterRCNNVGG16(FasterRCNN):\n",
    "    \"\"\"Faster R-CNN based on VGG-16.\n",
    "    For descriptions on the interface of this model, please refer to\n",
    "    :class:`model.faster_rcnn.FasterRCNN`.\n",
    "    Args:\n",
    "        n_fg_class (int): The number of classes excluding the background.\n",
    "        ratios (list of floats): This is ratios of width to height of\n",
    "            the anchors.\n",
    "        anchor_scales (list of numbers): This is areas of anchors.\n",
    "            Those areas will be the product of the square of an element in\n",
    "            :obj:`anchor_scales` and the original area of the reference\n",
    "            window.\n",
    "    \"\"\"\n",
    "\n",
    "    feat_stride = 16  # downsample 16x for output of conv5 in vgg16\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_fg_class=20,\n",
    "                 ratios=[0.5, 1, 2],\n",
    "                 anchor_scales=[8, 16, 32]\n",
    "                 ):\n",
    "                 \n",
    "        extractor, classifier = decom_vgg16()\n",
    "\n",
    "        rpn = RegionProposalNetwork(\n",
    "            512, 512,\n",
    "            ratios=ratios,\n",
    "            anchor_scales=anchor_scales,\n",
    "            feat_stride=self.feat_stride,\n",
    "        )\n",
    "\n",
    "        head = VGG16RoIHead(\n",
    "            n_class=n_fg_class + 1,\n",
    "            roi_size=7,\n",
    "            spatial_scale=(1. / self.feat_stride),\n",
    "            classifier=classifier\n",
    "        )\n",
    "\n",
    "        super(FasterRCNNVGG16, self).__init__(\n",
    "            extractor,\n",
    "            rpn,\n",
    "            head,\n",
    "        )\n",
    "\n",
    "class VGG16RoIHead(nn.Module):\n",
    "    \"\"\"Faster R-CNN Head for VGG-16 based implementation.\n",
    "    This class is used as a head for Faster R-CNN.\n",
    "    This outputs class-wise localizations and classification based on feature\n",
    "    maps in the given RoIs.\n",
    "    \n",
    "    Args:\n",
    "        n_class (int): The number of classes possibly including the background.\n",
    "        roi_size (int): Height and width of the feature maps after RoI-pooling.\n",
    "        spatial_scale (float): Scale of the roi is resized.\n",
    "        classifier (nn.Module): Two layer Linear ported from vgg16\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_class, roi_size, spatial_scale,\n",
    "                 classifier):\n",
    "        # n_class includes the background\n",
    "        super(VGG16RoIHead, self).__init__()\n",
    "\n",
    "        self.classifier = classifier\n",
    "        self.cls_loc = nn.Linear(4096, n_class * 4)\n",
    "        self.score = nn.Linear(4096, n_class)\n",
    "\n",
    "        normal_init(self.cls_loc, 0, 0.001)\n",
    "        normal_init(self.score, 0, 0.01)\n",
    "\n",
    "        self.n_class = n_class\n",
    "        self.roi_size = roi_size\n",
    "        self.spatial_scale = spatial_scale\n",
    "        self.roi = RoIPool( (self.roi_size, self.roi_size),self.spatial_scale)\n",
    "\n",
    "    def forward(self, x, rois, roi_indices):\n",
    "        \"\"\"Forward the chain.\n",
    "        We assume that there are :math:`N` batches.\n",
    "        Args:\n",
    "            x (Variable): 4D image variable.\n",
    "            rois (Tensor): A bounding box array containing coordinates of\n",
    "                proposal boxes.  This is a concatenation of bounding box\n",
    "                arrays from multiple images in the batch.\n",
    "                Its shape is :math:`(R', 4)`. Given :math:`R_i` proposed\n",
    "                RoIs from the :math:`i` th image,\n",
    "                :math:`R' = \\\\sum _{i=1} ^ N R_i`.\n",
    "            roi_indices (Tensor): An array containing indices of images to\n",
    "                which bounding boxes correspond to. Its shape is :math:`(R',)`.\n",
    "        \"\"\"\n",
    "        # in case roi_indices is  ndarray\n",
    "        roi_indices = at.totensor(roi_indices).float()\n",
    "        rois = at.totensor(rois).float()\n",
    "        indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n",
    "        # NOTE: important: yx->xy\n",
    "        xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
    "        indices_and_rois =  xy_indices_and_rois.contiguous()\n",
    "\n",
    "        pool = self.roi(x, indices_and_rois) #각 이미지 x에 맞게 roi pooling\n",
    "        pool = pool.view(pool.size(0), -1) #flatten\n",
    "        fc7 = self.classifier(pool) #fully connected\n",
    "        roi_cls_locs = self.cls_loc(fc7) #regression\n",
    "        roi_scores = self.score(fc7) #sofmax\n",
    "        return roi_cls_locs, roi_scores\n",
    "\n",
    "\n",
    "def normal_init(m, mean, stddev, truncated=False):\n",
    "    \"\"\"\n",
    "    weight initalizer: truncated normal and random normal.\n",
    "    \"\"\"\n",
    "    # x is a parameter\n",
    "    if truncated:\n",
    "        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)  # not a perfect approximation\n",
    "    else:\n",
    "        m.weight.data.normal_(mean, stddev)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRCNNVGG16(\n",
      "  (extractor): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (score): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (loc): Conv2d(512, 36, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (head): VGG16RoIHead(\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "    (cls_loc): Linear(in_features=4096, out_features=84, bias=True)\n",
      "    (score): Linear(in_features=4096, out_features=21, bias=True)\n",
      "    (roi): RoIPool(output_size=(7, 7), spatial_scale=0.0625)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "faster_rcnn = FasterRCNNVGG16()\n",
    "print(faster_rcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainer = FasterRCNNTrainer(faster_rcnn).cuda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dhk",
   "language": "python",
   "name": "dhk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
